[{"path":"index.html","id":"about","chapter":" 1 About","heading":" 1 About","text":"online resource created maintained Matteo Lisi designed collective repository staff students Department Psychology Royal Holloway, University London. contains miscellaneous collection tutorials, examples, case studies, workshop materials, variety additional content focusing data analysis modelling. materials updated expanded regularly response frequent questions requests received.pdf version available link.\\[\\\\[1in]\\]\nwork progress may contain imprecisions typos. spot please let know matteo.lisi [] rhul.ac.uk.\nmaterials included builds upon draw existing literature statistics modelling. endeavor properly cite existing books papers; author feels given fair acknowledgement, please let know make amend. book licenbsed CC -SA 4.0 license.\n","code":""},{"path":"surveys.html","id":"surveys","chapter":" 2 Departmental survey about statistical methods","heading":" 2 Departmental survey about statistical methods","text":"used anonymous survey ask colleagues questions topics may interesting useful research.","code":""},{"path":"surveys.html","id":"march-2022","chapter":" 2 Departmental survey about statistical methods","heading":"2.1 March 2022","text":"","code":""},{"path":"surveys.html","id":"question-1","chapter":" 2 Departmental survey about statistical methods","heading":"2.1.1 Question 1","text":"first question people indicated topics interests. winner multilevel models, followed closely Bayesian statistics.additional suggestions.topics mentioned comment section:Shiny appsNetwork modellingLongitudinal analysesRandom forestsNeural network","code":"#> [1] \"power analyses using Shiny apps\"                                                         \n#> [2] \"agent-based models\"                                                                      \n#> [3] \"this may be covered in the above, but approaches to analysing experience sampling method\"\n#> [4] \"Methods for longitudinal analyses\"                                                       \n#> [5] \"Network modelling\"                                                                       \n#> [6] \"Neural networks, Markov processes\"                                                       \n#> [7] \"Random forests and related\"                                                              \n#> [8] \"causal modelling using regression models - path models etc\"                              \n#> [9] \"prediction modelling\""},{"path":"surveys.html","id":"question-2","chapter":" 2 Departmental survey about statistical methods","heading":"2.1.2 Question 2","text":"people indicated interest topics related data analysis.things mentioned comments :SPMDockerPython","code":""},{"path":"surveys.html","id":"question-4","chapter":" 2 Departmental survey about statistical methods","heading":"2.1.3 Question 4","text":"question likelihood using different formats support","code":""},{"path":"surveys.html","id":"respondents-status","chapter":" 2 Departmental survey about statistical methods","heading":"2.1.4 Respondents’ status","text":"final questions asked status / career level.","code":""},{"path":"intro-R.html","id":"intro-R","chapter":" 3 Introduction to R","heading":" 3 Introduction to R","text":"practical statistical tutorials recipes book use software R, section provides introduction R uninitiated.\\[\\\\[1in]\\]","code":""},{"path":"intro-R.html","id":"installing-r","chapter":" 3 Introduction to R","heading":"3.1 Installing R","text":"base R system can downloaded following link, provides installers Windows, Mac Linux:https://cran.rstudio.com/addition base R system, useful also R-studio, IDE (Integrated Development Environment) R, provides editor, graphical interface much . can downloaded :https://www.rstudio.com/products/rstudio/download/","code":""},{"path":"intro-R.html","id":"first-steps","chapter":" 3 Introduction to R","heading":"3.2 First steps","text":"R programming language free software environment statistical computing graphics. interpreted language, means give instructions computer compile first machine language, everything done ‘fly’ command line interpreter, e.g. type 2+2 command line R, computer reply answer (try computer):Typically normal workflow involve writing saving series instructions script file (usually saved .R extension), can executed (either step step ). Since steps analyes documented script, makes transparent reproducible.R script can use # sign add comments, others can understand R code . Commented lines ignored R, influence result. See next example:","code":"\n2+2\n#> [1] 4\n# calculate 3 + 4\n3 + 4\n#> [1] 7"},{"path":"intro-R.html","id":"arithmetic-with-r","chapter":" 3 Introduction to R","heading":"3.2.1 Arithmetic with R","text":"basic form, R can used simple calculator. Consider following arithmetic operators:Addition: +Subtraction: -Multiplication: *Division: /Exponentiation: ^Modulo: %%last two might need explaining:^ operator raises number left power number right: example 3^2 9.modulo returns remainder division number left number right, example 5 modulo 3 (5 %% 3) 2.","code":""},{"path":"intro-R.html","id":"variable-assignment","chapter":" 3 Introduction to R","heading":"3.2.2 Variable assignment","text":"basic concept programming (statistical ) called variable.variable allows store value (e.g. 2) object (e.g. function description) R. can later use variable’s name easily access value object stored within variable.can assign value 2 variable my_var commandNote obtained result using:, assignment operator works directions <- ->.variable can used computation, example:","code":"\nmy_var <- 2\n2 -> my_var\nmy_var + 2 \n#> [1] 4"},{"path":"intro-R.html","id":"basic-data-types-in-r","chapter":" 3 Introduction to R","heading":"3.2.3 Basic data types in R","text":"Variables can many types, just numerical values. example, can contain text values (e.g. string characters). Arithmetic operators + work . tried apply characters R give error message.fact R works numerous data types, numerical (can’t added, subtracted, etc.). basic types get started :Decimal values like 4.5 called numerics.Natural numbers like 4 called integers. Integers also numerics.Boolean values (TRUE FALSE, abbreviated T F) called logical1.Text (string) values called characters.","code":"\n# Assign a value to the variable apples\napples <- 5 \n\n# Assign a text value\noranges <- \"six\" \n\n#  \napples + oranges \n#> Error in apples + oranges: non-numeric argument to binary operator"},{"path":"intro-R.html","id":"vectors-and-other-data-types","chapter":" 3 Introduction to R","heading":"3.2.4 Vectors and other data types","text":"Additionally, simple data types listed can combined complex ‘objects’ can comprise several values. example, can obtain vector concatenating values using function c(). can applied numerical character data types, e.g. special handy functions create specific types vectors, sequences (using function seq() operator :)Another useful type vector can obtained repetition elements, can numerical, character, even applied vectorsWe can combine vectors different types data frame, one useful ways storing data R. Let’s say 3 vectors:can combine data.frame using:Although note cases probably import dataframe external data file, example using functions read.table read.csv.","code":"\nsome_numbers <- c(4,87,10, 0.5, -6)\nsome_numbers\n#> [1]  4.0 87.0 10.0  0.5 -6.0\n\nmy_modules <- c(\"PS115\", \"PS509\", \"PS300\", \"PS938\", \"PS9457\")\nmy_modules\n#> [1] \"PS115\"  \"PS509\"  \"PS300\"  \"PS938\"  \"PS9457\"\nx <- seq(from = -10, to = 10, by = 2)\nx\n#>  [1] -10  -8  -6  -4  -2   0   2   4   6   8  10\n\ny <- seq(-0, 1, 0.1)\ny\n#>  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\nz <- 1:5\nz\n#> [1] 1 2 3 4 5\nrep(3, 5)\n#> [1] 3 3 3 3 3\n\nx <- 1:3\nrep(x, 4)\n#>  [1] 1 2 3 1 2 3 1 2 3 1 2 3\n\nrep(c(\"leo the cat\", \"daisy the dog\"), 2)\n#> [1] \"leo the cat\"   \"daisy the dog\" \"leo the cat\"  \n#> [4] \"daisy the dog\"\n# create a numeric vector \na <- c(0, NA, 2:4)  # NA means not available\n\n# create a character vector\nb <-  c(\"PS115\", \"PS509\", \"PS300\", \"PS938\", \"PS9457\")\n\n# create a logical vector\nc <- c(TRUE, FALSE, TRUE, FALSE, FALSE)  # must all be caps!\n# create a data frame with the vectors a, b,and c that we just created\nmy_dataframe <- data.frame(a,b,c)\n\n# we could also change the column names (currently they are a, b, c)\ncolnames(my_dataframe) <- c(\"some_numbers\", \"my_modules\", \"logical_values\")\n\n# now let's have a look at it\nmy_dataframe\n#>   some_numbers my_modules logical_values\n#> 1            0      PS115           TRUE\n#> 2           NA      PS509          FALSE\n#> 3            2      PS300           TRUE\n#> 4            3      PS938          FALSE\n#> 5            4     PS9457          FALSE"},{"path":"intro-R.html","id":"basic-plotting-in-r","chapter":" 3 Introduction to R","heading":"3.2.5 Basic plotting in R","text":"can create plots using function plot(). example:","code":"\nx = 1:10\ny = 3*x - 5\nplot(x, y)"},{"path":"intro-R.html","id":"other-operations","chapter":" 3 Introduction to R","heading":"3.2.6 Other operations","text":"","code":""},{"path":"intro-R.html","id":"random-number-generation","chapter":" 3 Introduction to R","heading":"3.2.6.1 Random number generation","text":"Generate uniformly distributed random numbers (function runif())Generate numbers normal distribution","code":"\nx <- runif(100, min = 0, max = 1)\nhist(x)\ny <- rnorm(100, mean = 0, sd = 1)\nhist(y)"},{"path":"intro-R.html","id":"getting-help","chapter":" 3 Introduction to R","heading":"3.2.7 Getting help","text":"R lot functions, extra packages can provides even . may seem bit overwhelming, easy get help use function: just type question mark, followed name function. example, see help function used generate histogram, type\\[\\\\[1in]\\]","code":"\n?hist"},{"path":"intro-R.html","id":"resources-for-learning-r","chapter":" 3 Introduction to R","heading":"3.3 Resources for learning R","text":"plenty resources web learn R. recommend couple think particularly well-done useful:Software Carpentry tutorials R Reproducible Scientific AnalysisThe free book Learning Statistics R Danielle Navarro","code":""},{"path":"correlation.html","id":"correlation","chapter":" 4 Correlations","heading":" 4 Correlations","text":"Readers page likely already familiar Pearson correlation coefficient significance test. Pearson correlation coefficient measures strength association two variables, denoted \\(x\\) \\(y\\), essentially normalized measure covariance. calculated dividing covariance \\(x\\) \\(y\\) product standard deviations:\\[\nr_{x,y} = \\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y}\n\\]normalization achieved dividing covariance maximum possible covariance can attained variables perfectly correlated, represented product standard deviations. normalization constrains Pearson correlation coefficient range -1 1.R, simple correlation analyses can run using function cor.test.chapter consider complex examples issues may occur correlation analyses.","code":""},{"path":"correlation.html","id":"comparing-correlations","chapter":" 4 Correlations","heading":"4.1 Comparing correlations","text":"Often, compare correlation value two conditions (groups whatever). One simple approach - access raw data - run regression model (interaction term) condition categorical predictor, test whether slope differs two conditions. (Note variables standardized - .e. transformed Z-scores - regression slope identical Pearson correlation coefficient.)don’t raw data available, can still compare correlations using Fisher’s Z transform. requires first transforming correlation values, e.g. \\(r_1, r_2\\), Z scores using following:\\[\nZ=\\frac{\\log(1+r)−\\log(1−r)}{2}\n\\]can take differences transformed correlation coefficients, divide \\(\\sqrt{\\frac{1}{N_1 - 3} + \\frac{1}{N_2 - 3}}\\), \\(N_1\\) \\(N_2\\) sample size two conditions/groups compared. obtain statistics\\[\n\\Delta_Z = \\frac{|Z_1 - Z_2|}{\\sqrt{\\frac{1}{N_1 - 3} + \\frac{1}{N_2 - 3}}}\n\\]null hypothesis two correlations different (\\(H_0: r_1 =r_2\\)) statistics normally distributed mean 0 variance 1, \\(\\Delta_Z \\sim \\mathcal{N}(0,1)\\). Thus can get p value using cumulative distribution function normal distribution. example, R statistic saved variable Delta_Z can calculate p-value using p_value <- 2*(1-pnorm(Delta_Z)).verify correctness approach, can perform simple simulation R. code simulates multiple pairs datasets, consisting two variables, population given correlation. simulation, ‘true’ correlation value remains constant, representing case null hypothesis true. use aforementioned approach determine whether difference statistically significant. Finally, calculate proportion significant tests (.e., false positives) verify approximately equal desired false positive rate (alpha), conventionally set 0.05.","code":"\n# function to simulate correlated normal variables\nsim_data <- function(N=100, r=0.3){\n  d <- MASS::mvrnorm(N, \n               mu = c(0,0), \n               Sigma=matrix(c(1,r,r,1), nrow=2, ncol=2))\n  return(data.frame(d))\n}\n\n# function that calculate F transform and statistics\nFisher_Z_test <- function(r1, r2, N1, N2){\n  Z1 <- (log(1+r1) - log(1-r1))/2\n  Z2 <- (log(1+r2) - log(1-r2))/2\n  denomin <- sqrt(1/(N1-3) + 1/(N2-3))\n  Delta_Z <- abs(Z1 - Z2)/denomin\n  p_value <- 2*(1-pnorm(Delta_Z))\n  return(list(statistic=Delta_Z, p=p_value))\n}\n\n# run simulations\nset.seed(1)\nN_sim <- 1e4\np_vals <- rep(NA, N_sim)\nfor(i in 1:N_sim){\n  d1 <- sim_data()\n  d2 <- sim_data()\n  r1 <- cor(d1$X1, d1$X2)\n  r2 <- cor(d2$X1, d2$X2)\n  test.results <- Fisher_Z_test(r1, r2, 100, 100)\n  p_vals[i] <- test.results$p\n}\n\n# false positive rate\n# should be approximately equal to desired alpha\n# (here alpha=0.05)\nmean(p_vals<0.05)\n#> [1] 0.0505"},{"path":"correlation.html","id":"polychoric-and-polyserial-correlations","chapter":" 4 Correlations","heading":"4.2 Polychoric and polyserial correlations","text":"Polychoric polyserial correlations methods used estimate correlations ordinal variables ordinal continuous variables. methods assume ordinal variables can considered divisions underlying latent variables follow normal distribution (similar ordinal models discussed later chapter). R, calculating polychoric correlations straightforward help polycor package. can use polychor function estimate correlations two variables hetcor function compute correlation matrix multiple variables simultaneously.","code":""},{"path":"correlation.html","id":"partial-correlations","chapter":" 4 Correlations","heading":"4.3 Partial correlations","text":"Partial correlation statistical measure quantifies relationship two variables controlling effects variables. assesses strength direction linear association two variables removing influence remaining variables analysis.Partial correlations useful situations multiple variables may interconnected, want understand relationship two variables accounting effects variables. calculating partial correlations, can examine direct association two variables holding constant influence variables, thereby revealing unique relationship .scenarios partial correlations can useful:Confounding Control: observational studies, may confounding variables affect dependent independent variables. computing partial correlations, can determine relationship variables interest controlling potential confounders.Confounding Control: observational studies, may confounding variables affect dependent independent variables. computing partial correlations, can determine relationship variables interest controlling potential confounders.Multivariate Analysis: examining relationships multiple variables simultaneously, partial correlations can help identify direct associations pairs variables, accounting shared influence variables analysis.Multivariate Analysis: examining relationships multiple variables simultaneously, partial correlations can help identify direct associations pairs variables, accounting shared influence variables analysis.Network Analysis: Partial correlations commonly employed network analysis uncover underlying structure connections variables, gene regulatory networks, social networks, financial networks.Network Analysis: Partial correlations commonly employed network analysis uncover underlying structure connections variables, gene regulatory networks, social networks, financial networks.R, simple Pearson’s correlation coefficients, can use packages ppcor calculate matrix partial correlation coefficients.","code":""},{"path":"correlation.html","id":"partial-correlations-using-schur-complement","chapter":" 4 Correlations","heading":"4.3.1 Partial correlations using Schur complement","text":"Occasionally, may need calculate partial polychoric correlations (e.g. partial correlations ordinal variables). best knowledge isn’t simple package allows compute . However, can use approach known Schur complement.approach works follow. Say computed matrix partial polychoric correlations using polycor package; particular \\(\\Sigma\\) correlation matrix variables interests. also set variables want account (let’s call ‘confounders’), calculate matrix correlations , let’s call \\(C\\). Finally, also matrix correlations variables interest confounders, notated \\(B\\).partial correlation matrix can computed Schur complement \\(C\\) block matrix \\(M\\), defined \\(M = \\begin{bmatrix} \\Sigma & B \\\\ B^T & C \\end{bmatrix}\\). Note matrix \\(M\\) simply corresponds ‘full’ correlation matrix - matrix including correlations variables (variables interest confounders).practice, Schur complement (partial correlation matrix) computed \\[\\text{Partial Correlation Matrix} = \\Sigma - BC^{-1}B^T.\\]simple function R can calculate partial correlation matrix variables ordinal:estimate p-values partial correlation coefficients, can use permutation test: essentially shuffle order columns dataset independently, re-calculate correlation matrix, keep track often correlation higher observed ones occur chance.code example test can implemented R","code":"\npartial_polychoric <- function(dX, dC, useML = TRUE) {\n  # dX is a dataframe containing variables of interest\n  # dC is a dataframe containing confounding variables that needs to be partialled out\n  # ML estimation is recommended, but may be slow with very large datasets\n  combined_data <- cbind(dX, dC)\n  combined_polychoric_matrix <- hetcor(combined_data, \n                                       parallel=TRUE,\n                                       ML=useML,\n                                       use=\"pairwise.complete.obs\")$correlations\n  cor_dX <- combined_polychoric_matrix[1:ncol(dX), 1:ncol(dX)]\n  cor_dC <- combined_polychoric_matrix[1:ncol(dX), (ncol(dX) + 1):ncol(combined_data)]\n  cor_yy <- combined_polychoric_matrix[(ncol(dX) + 1):ncol(combined_data), (ncol(dX) + 1):ncol(combined_data)]\n  inv_cor_yy <- solve(cor_yy)\n  cor_dX - cor_dC %*% inv_cor_yy %*% t(cor_dC)\n}\n# first, compute the observed matrix of partial correlation\nsigma_polychoric <- partial_polychoric(dX, dC)\n\n# custom function to shuffle independently the columns \n# of a data.frame containing ordinal variables\nshuffle_df <- function(A) {\n  A_shuffle <-  as.data.frame(lapply(A, function(x) {\n    if (is.ordered(x)) {\n      levels <- levels(x)\n      x <- as.numeric(x)\n      x <- sample(x)\n      x <- ordered(x, labels = levels)\n    } else {\n      x <- sample(x)\n    }\n    return(x)\n  }))\n  return(A_shuffle)\n}\n\n# Use replicate() to compute 1000 matrix from permutations of the datasets\nn_permutations <- 1000\npermuted_partial_polychoric <- replicate(n_permutations, {\n  permuted_dX <- shuffle_df(dX)\n  permuted_dC <- shuffle_df(dC)\n  partial_polychoric(permuted_dX, permuted_dC)\n})\n\n# Compute p-values by counting how often we observe correlations \n# as high as that in the observed matrix from the permuted datasets.\n# The output is a mtrix of p-values\np_values <- matrix(0, ncol(dX), ncol(dX))\ntot_it <- ncol(dX)*ncol(dX)\nfor (i in 1:ncol(dX)) {\n  for (j in 1:ncol(dX)) {\n    p_values[i, j] <- mean(abs(permuted_partial_polychoric[i, j, ]) >= abs(observed_partial_polychoric[i, j]))\n  }\n}"},{"path":"linear-models.html","id":"linear-models","chapter":" 5 Linear models","heading":" 5 Linear models","text":"section provide worked examples analyses R.","code":""},{"path":"linear-models.html","id":"simple-linear-regression","chapter":" 5 Linear models","heading":"5.1 Simple linear regression","text":"example2 see import data R perform simple linear regression analysis.According standard big-bang model, universe expands uniformly locally, according Hubble’s law3\n\\[\n\\text{velocity} = \\beta \\times \\text{distance}\n\\]\n\\(\\text{velocity}\\) \\(\\text{distance}\\) relative velocity distance galaxy, respectively; \\(\\beta\\) “Hubble’s constant”4. Note simple linear equation, \\(\\beta\\) indicate much variable \\(\\text{velocity}\\) changes unitary increase variable \\(\\text{distance}\\).According model \\(\\beta^{-1}\\) gives approximate age universe, \\(\\beta\\) unknown must somehow estimated observations \\(\\text{velocity}\\) \\(\\text{distance}\\), made variety galaxies different distances us. Luckily available data Hubble Space Telescope. Velocities assessed measuring Doppler effect red shift spectrum light receive Galaxies. Distance estimated indirectly, using discovery certain class stars (Cepheids), display fluctuations diameter temperature stable period, systematic relationship period luminosity.can load dataset measurements Hubble Space Telescope R using following coderead.table generic function import dataset text files (e.g. .csv files) R. use argument header=T specify first line dataset gives names columns. Note argument file URL, also path file local folder. see help function, arguments features available type ?read.table R command line.can use command str() examine importedThis tells us data frame 3 variables:Galaxy, ‘names’ galaxies datasetvelocity, relative velocity Km/secdistance, distance expressed Mega-parsecs5We can plot6 using following code:clear, figure, observed data follow Hubble’s law exactly, given measurements obtained (uncertainty true values distance velocities) surprising . Given apparent variability, can inferred data?\nparticular:value \\(\\beta\\) consistent data?range \\(\\beta\\) values consistent data?order make inferences make assumptions nature measurement noise. Specifically, assume measurements errors well-characterized Gaussian distribution. result following model:\n\\[\\begin{align*}\ny &= \\beta x + \\epsilon \\\\\n\\epsilon &\\sim \\mathcal{N} \\left(0, \\sigma_{\\epsilon}^2 \\right)\n\\end{align*}\\]\nessentially linear regression without intercept: , whereas normally linear regression model include additive term multiplied predictor (\\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)), gives expected value dependent variable predictors set zero, case theory tells us can assume intercept (term \\(\\beta_0\\)) zero can ignore .can fit model function lm R. Note tell R don’t want fit intercept, include formula term 0 + - essentially tells R intercept term set zero7So, based data, estimate Hubble constant 76.58 standard error 3.96. standard error - standard deviation sampling distribution estimates - gives ideas range values compatible data used compute confidence intervals (roughly, expect ‘true’ values parameters lies interval defined \\(\\pm\\) 2 standard errors 95% times)., old universe?Hubble constant estimate units \\(\\frac{\\text{Km}/\\text{sec}}{\\text{Mega-parsecs}}\\). Mega-parsecs \\(3.09 \\times 10^{19} \\text{Km}\\), divide estimate \\(\\hat \\beta\\) amount. reciprocal \\(\\hat \\beta\\) gives approximate age universe (seconds). R can calculate (years) followgiving estimate 13 billion years.","code":"\nd <- read.table(file=\"https://raw.githubusercontent.com/mattelisi/RHUL-stats/main/data/hubble.txt\", \n                header=T)\nstr(d)\n#> 'data.frame':    24 obs. of  3 variables:\n#>  $ Galaxy  : chr  \"NGC0300\" \"NGC0925\" \"NGC1326A\" \"NGC1365\" ...\n#>  $ velocity: int  133 664 1794 1594 1473 278 714 882 80 772 ...\n#>  $ distance: num  2 9.16 16.14 17.95 21.88 ...\nplot(d$distance, # indicate which variable on X axis\n     d$velocity, # indicate which variable on Y axis\n     xlab=\"Distance [Mega-parsecs]\",\n     ylab=\"Velocity [Km/sec]\", \n     pch=19) # set the type of point\nhub.m <- lm(velocity ~ 0 + distance, d)\nsummary(hub.m)\n#> \n#> Call:\n#> lm(formula = velocity ~ 0 + distance, data = d)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -736.5 -132.5  -19.0  172.2  558.0 \n#> \n#> Coefficients:\n#>          Estimate Std. Error t value Pr(>|t|)    \n#> distance   76.581      3.965   19.32 1.03e-15 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 258.9 on 23 degrees of freedom\n#> Multiple R-squared:  0.9419, Adjusted R-squared:  0.9394 \n#> F-statistic: 373.1 on 1 and 23 DF,  p-value: 1.032e-15\n# transform in Km\nhubble.const <- coef(hub.m)/(3.09 * 10^(19))\n\n# invert to get age in seconds\nage <- 1/hubble.const\n\n# use unname() to avoid carrying over \n# the label \"distance\" from the model\nage <- unname(age)\n\n# transform age in years\nage <- age/(60^2 * 24 * 365)\n\n# age in billion years\nage/10^9\n#> [1] 12.79469"},{"path":"count-data.html","id":"count-data","chapter":" 6 Models for count data","heading":" 6 Models for count data","text":"section provide examples models can deal count data. Typically, count data occurs dependent variable counted number occurrences event - example number patients arriving emergency department (&E) given time day - e.g. 10:00 11:00. case, dependent variable (number patients) several characteristics make unsuitable analysis standard linear models linear regression: distribution discrete, composed non-negative integers, often positively skewed, many observations value 0.Another characteristic variance observations (e.g. variance number counts across observations within condition) increases expected value (e.g. average number counts condition)8.","code":""},{"path":"count-data.html","id":"poisson-model","chapter":" 6 Models for count data","heading":"6.1 Poisson model","text":"simplest model account characteristics mentioned generalized linear model assume dependent variable Poisson distribution. Poisson distribution single free parameter, usually notated \\(\\lambda\\), gives expected value (mean) variance count variable. fact assumes variance value mean.Formally, Poisson model usually formulated follow: let \\(y=(y_1, \\dots, y_n)\\) dependent variable, consisting counts (non-negative integers) \\(x\\) independent variable (predictor). \\[\ny_i \\sim \\text{Poisson} \\left(\\lambda_i \\right)\n\\]\nwhere9\n\\[\n\\log( \\lambda_i) = \\beta_0 + \\beta_1 x_i\n\\]\nalternatively\n\\[\n\\lambda_i = e^{\\beta_0 + \\beta_1 x_i}\n\\]\nindicates can use exponential function (R function exp()) calculate predicted values mean (precisely, expected value, \\(\\mathbb{E}(y)\\)) variance (\\(Var(y)\\)) dependent variable. relies property Poisson distribution10\n\\[\ny_i \\sim \\text{Poisson} \\left(\\lambda \\right) \\implies \\mathbb{E}(y)=Var(y)=\\lambda\n\\]However, practice, data often conform constraint identical mean variance. Often observed variance count higher predicted according Poisson model (say case data -dispersed).-dispersion look like?type data expected Poisson model illustrated figure , shows 100 datapoints simulated model \\(y_i \\sim \\text{Poisson} \\left(e^{1 + 2x_i }\\right)\\). vertical deviations datapoints line consistent property Poisson distribution variance hte count value expected value, formally, \\(Var(y) = \\mathbb{E}(y)\\)).can adjust code simulate data degree -dispersion, using negative binomial distribution, different values precision parameter theta (\\(\\theta\\)), regulate degree overdispersion. Importantly, datapoints simulated assuming function fo average (expected) number counts (also previous figure), just differe amount overdispersion relative Poissone model. Note value arbitrarily large precision parameter \\(\\theta \\rightarrow \\infty\\) (bottom-right panel) negative binomial converges Poisson.","code":"\nset.seed(2)\nn <- 100\nx <- runif(n, -1.3, 1.3)\na <- 1\nb <- 2\nlinpred <- a + b*x # linear predictor part\ny <- rpois(n, exp(linpred)) # simulate Poisson observations\n\nplot(x,y,col=\"blue\") # plot\nx_ <- seq(-2,2,0.1)\nlines(x_, exp(a+b*x_))\nsegments(x,exp(a+b*x),x,y, lwd=1,lty=3, col=\"red\")"},{"path":"count-data.html","id":"negative-binomial-model","chapter":" 6 Models for count data","heading":"6.2 Negative binomial model","text":"mentioned , data often overdispersed relative Poisson (, variance larger mean). issue data overdispersed results may largely influences extreme datapoints. Moreover, wrong estimated variability data. account overdispersion can use negative binomial can seen generalization Poisson model11.Formally, negative binomial model 1 predictor \\(x\\) can notated \n\\[\n\\begin{aligned}\ny_i & \\sim \\text{NegBinomial} \\left(\\lambda_i, \\theta \\right)\\\\\n\\log( \\lambda_i) & = \\beta_0 + \\beta_1 x_i\n\\end{aligned}\n\\]negative binomial similar Poisson - particular still case \\(\\mathbb{E}(y)=\\lambda\\). However, includes additional precision (“reciprocal dispersion”) parameter referred \\(\\theta\\)12. Essentially, whereas Poisson \\(Var(y) = \\mathbb{E}(y)\\), now \n\\[\nVar(y) = \\mathbb{E}(y) + \\frac{\\mathbb{E}(y)^2}{\\theta}\n\\]","code":""},{"path":"count-data.html","id":"examples","chapter":" 6 Models for count data","heading":"6.3 Examples","text":"","code":""},{"path":"count-data.html","id":"anchoring-and-alcohol-units","chapter":" 6 Models for count data","heading":"6.3.1 Anchoring and alcohol units","text":"see work practice, use negative binomial analyse dataset based experiment run Ryan McKay MSc students13.goal study use anchoring effect :…see attenuate -reporting alcohol consumption (medical problem). Participants high-anchor condition asked “drink less 40 units alcohol week”, asked estimate exactly many units ’d consumed. low anchor condition initially asked “drink less 4 units alcohol week” giving precise estimate, control condition just gave precise estimate.R begin loading dataWe can calculate mean variance number units reported conditions. reveal variance across participants number units reported many times higher mean number reported units.\nTable 6.1: Mean Variance weekly units alcohol reported.\ncan use ggplot2 library visualize distributions reported units condition. can see tha distribution skewed contains many 0, make unsuitable analysis linear regression model.estimate negative-binomial model, can use function glm.nb() available MASS package. predictor condition categorical 3 levels therefore coded internally set 2 dummy variables. can see contrast set usingThis indicate control baseline condition model 2 coefficients code difference high low anchoring condition relative control one.Note also analysis variable units must contain integer values - participants reported non-integer values (e.g. bottle lager 1.7 units) divide everything minimum common denominator end integer values.following command can used estimate model examine resultsWe can see output condition high anchoring elicited reports higher number alcohol units control condition.can use model make precise statement size difference. can use value coefficients calculate predicted values counts. exact values coefficients can accessed fitted model using $ operatorThe values combined together according dummy variables coding condition represents linear predictor part model:\\[\n\\lambda_i = \\exp \\left(\\beta_0 + \\beta_1\\times D_\\text{high}+ \\beta_2\\times D_\\text{low} \\right)\n\\]\nused notation \\(D_\\text{high}\\) \\(D_\\text{low}\\) indicate two dummy variable, whose value 1 observation high low conditions, respectively, zero otherwise.\\(\\beta_0\\) common notation intercept parameter - case gives expected number alcohol units control condition (observations control condition \\(D_\\text{high}= D_\\text{low}=0\\)). Thus model predict average number counts control condition (Compare value table ).Furthermore, models tells us also number reported alcohol units increase multiplicatively high condition factor ofIn fact predicted number counts high condition can derived model asor equivalentlyFinally, note can use sjPlot library prepare fancy version model output, can see multiplicative factor describe increase reported units called incidence ratio14.","code":"\nd <- read_csv(\"../data/nb_units.csv\", show_col_types = F) # data availabel in the data folder of the repository\nd\n#> # A tibble: 930 × 3\n#>    gender condition units\n#>    <chr>  <chr>     <dbl>\n#>  1 Male   high         10\n#>  2 Male   low           0\n#>  3 Female high          1\n#>  4 Female high          2\n#>  5 Male   high          0\n#>  6 Female low           2\n#>  7 Male   control      23\n#>  8 Female low           2\n#>  9 Female high          1\n#> 10 Male   control       4\n#> # ℹ 920 more rows\nd %>%\n  group_by(condition) %>%\n  summarise(Mean = mean(units),\n            Variance = var(units)) %>%\n  knitr::kable(digits=2,\n               caption=\"Mean and Variance of weekly units of alcohol reported.\")\nd %>%\n  ggplot(aes(x=units, fill=condition)) +\n  geom_histogram(binwidth=1)+\n  facet_grid(condition~.) +\n  theme_minimal() \nd$condition <- factor(d$condition) # tell R that this is a categorical factor\ncontrasts(d$condition)\n#>         high low\n#> control    0   0\n#> high       1   0\n#> low        0   1\nlibrary(MASS)\n#> \n#> Attaching package: 'MASS'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     select\nnb01 <- glm.nb(units ~ condition, data = d)\nsummary(nb01)\n#> \n#> Call:\n#> glm.nb(formula = units ~ condition, data = d, init.theta = 0.3852966632, \n#>     link = log)\n#> \n#> Coefficients:\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)    2.01856    0.09396  21.482  < 2e-16 ***\n#> conditionhigh  0.46365    0.13218   3.508 0.000452 ***\n#> conditionlow   0.07564    0.13255   0.571 0.568251    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(0.3853) family taken to be 1)\n#> \n#>     Null deviance: 1040.0  on 929  degrees of freedom\n#> Residual deviance: 1025.2  on 927  degrees of freedom\n#> AIC: 5656.8\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  0.3853 \n#>           Std. Err.:  0.0196 \n#> \n#>  2 x log-likelihood:  -5648.7830\nnb01$coefficients\n#>   (Intercept) conditionhigh  conditionlow \n#>    2.01856406    0.46365080    0.07563937\nexp(nb01$coefficients[\"(Intercept)\"]) # equivalent to exp(nb01$coefficients[1])\n#> (Intercept) \n#>    7.527508\nexp(nb01$coefficients[\"conditionhigh\"])\n#> conditionhigh \n#>      1.589868\nexp(nb01$coefficients[\"(Intercept)\"])  * exp(nb01$coefficients[\"conditionhigh\"]) \n#> (Intercept) \n#>    11.96774\nexp(nb01$coefficients[\"(Intercept)\"] + nb01$coefficients[\"conditionhigh\"]) \n#> (Intercept) \n#>    11.96774\nlibrary(sjPlot)\n#> Install package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\ntab_model(nb01)"},{"path":"count-data.html","id":"adding-predictors","chapter":" 6 Models for count data","heading":"6.3.1.1 Adding predictors","text":"dataset include also information gender participants. may hypothesize male participants drink female ones15. taking account improve accuracy modelling?test , can estimate additional model also gender predictor. can compare previous one using likelihood-ratio test. based theorem states difference log-likelihood16 nested models (asymptotically) distributed according Chi-squared distribution, therefore allowing calculation p-value. R can done using function anova().First, let’s fit additional model extra predictor genderThis indicate indeed male participants report averagetimes units alcohol per week females.can already see difference due gender significant, nevertheless let’s compare using likelihood ratio test.value likelihood ratio statistic NA, 70.88 null hypothesis improvement model fit obtained adding gender predictor due chance distributed Chi-square 1 degree freedom.","code":"\nnb02 <- glm.nb(units ~ condition + gender, data = d)\nsummary(nb02)\n#> \n#> Call:\n#> glm.nb(formula = units ~ condition + gender, data = d, init.theta = 0.4212084665, \n#>     link = log)\n#> \n#> Coefficients:\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   1.495941   0.105452  14.186  < 2e-16 ***\n#> conditionhigh 0.426116   0.127159   3.351 0.000805 ***\n#> conditionlow  0.006701   0.127720   0.052 0.958156    \n#> genderMale    0.909721   0.103969   8.750  < 2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(0.4212) family taken to be 1)\n#> \n#>     Null deviance: 1115.5  on 929  degrees of freedom\n#> Residual deviance: 1025.4  on 926  degrees of freedom\n#> AIC: 5587.9\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  0.4212 \n#>           Std. Err.:  0.0219 \n#> \n#>  2 x log-likelihood:  -5577.9060\nexp(nb02$coefficients[\"genderMale\"])\n#> genderMale \n#>    2.48363\nanova(nb01, nb02)\n#> Likelihood ratio tests of Negative Binomial Models\n#> \n#> Response: units\n#>                Model     theta Resid. df    2 x log-lik.\n#> 1          condition 0.3852967       927       -5648.783\n#> 2 condition + gender 0.4212085       926       -5577.906\n#>     Test    df LR stat. Pr(Chi)\n#> 1                              \n#> 2 1 vs 2     1 70.87633       0"},{"path":"count-data.html","id":"plotting-model-fit","chapter":" 6 Models for count data","heading":"6.3.1.2 Plotting model fit","text":"’s straightforward visualize model fit data - code give one possibility:\nFigure 6.1: black line represent predicted probability data (note clipped x-axis 40)\nAdmittedly probability data model (black lines) looks quite similar across three panels, however, model assign higher probability higher count values high condition compared ones - can see putting together panel, plotting logarithm probability instead probability . changes probability data may seems large looked way, amount quite substantial changes average number counts - recall high condition participants reported average nearly 1.6 times number alcohol units control condition.","code":"\n# here I make a new data matric for claculating the prediction of the model\nnd <- expand.grid(condition=unique(d$condition), \n                  units = 0:max(d$units),\n                  KEEP.OUT.ATTRS = F)\n\n# use the predict() function to calculate the predicted counts for each condition\nnd$predicted_units <- predict(nb01, newdata=nd, type=\"response\")\n\n# here I use the dnbinom() function - which gives the probability density of \n# the negative binomial - to calculate the probability of the observations under the model\nnd$pred_density <- dnbinom(nd$units, mu=nd$predicted_units, size=nb01$theta)\n\n# finally take all together and plot\nd %>%\n  ggplot(aes(x=units, fill=condition)) +\n  geom_histogram(aes(y=..density..),binwidth=1, color=\"white\")+\n  geom_line(data=nd, aes(x=units, y=pred_density),size=1) +\n  facet_grid(condition~.) +\n  theme_minimal() +\n  coord_cartesian(xlim=c(0,40))\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2\n#> 3.4.0.\n#> ℹ Please use `linewidth` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where\n#> this warning was generated.\n#> Warning: The dot-dot notation (`..density..`) was deprecated in\n#> ggplot2 3.4.0.\n#> ℹ Please use `after_stat(density)` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where\n#> this warning was generated.\nnd %>%\n  ggplot(aes(x=units, y=log(pred_density), color=condition))+\n  geom_line(size=0.6)+\n  theme_minimal() +\n  coord_cartesian(xlim=c(0,40),ylim=c(-7,-1.4))+\n  labs(y=\"log (probability)\")"},{"path":"ordinal.html","id":"ordinal","chapter":" 7 Models for ordinal data","heading":" 7 Models for ordinal data","text":"Ordinal-type variable arise often psychology. One common example responses Likert scales. Although common practice analyzed linear model, know approach can lead serious inference errors.17 reason, recommended approach use model appropriate ordinal data. describe approach , using ordered logistic regression model (also know proportional odds model).","code":""},{"path":"ordinal.html","id":"ordered-logistic-regression","chapter":" 7 Models for ordinal data","heading":"7.1 Ordered logistic regression","text":"One way think model assuming existence continuous latent quantity, call \\(y\\), specified logistic probability density function. latent distribution partitioned series \\(k\\) intervals, \\(k\\) number ordered choice options available respondents, using \\(k+1\\) latent cut-points, \\(c_1, \\ldots, c_{k+1}\\). integrating latent density function within interval obtain ordinal response probabilities \\(p_1, \\ldots, p_k\\). choices possible (e.g. assuming normally distributed latent variable yield ordered probit model). Beyond mathematical convenience, one advantage ordered logit coefficient can interpreted ordered log-odds, implementing proportional odds assumption.18Formally, model can notated \\[\n\\begin{aligned}\np_k & = p\\left(c_{k-1} < y \\le c_k \\mid \\mu \\right)\\\\\n& = \\text{logit}^{-1}\\left(c_k - \\mu \\right) - \\text{logit}^{-1}\\left(c_{k-1} - \\mu \\right)\n\\end{aligned}\n\\]\n\\[\n\\text{logit}^{-1}(\\alpha) = \\frac{1}{1+e^{-\\alpha}}\n\\]\ncumulative function logistic distribution (also known inverse-logit), \n\\[\n\\mu = \\beta_1 x_1 + \\ldots + \\beta_n x_n\n\\]\nlinear part model (linear combination \\(n\\) predictor variables).general approach formalism used - present examples illustrates work practice R.","code":""},{"path":"ordinal.html","id":"mixed-effects-ordinal-regression","chapter":" 7 Models for ordinal data","heading":"7.1.1 Mixed-effects ordinal regression","text":"R libraries used exampleIn addition libraries, create handy R function gives probabilities categorical responses given mean value latent quantity (indicated \\(\\mu\\) ) set cutpoints \\(c_1, \\ldots, c_{k+1}\\). used simulating data plotting fit model.example simulate data. two predictors: x1, continuous predictor vary observation, d1 dummy variable indicate categorical predictor 2 levels (e.g. two experimental conditions). conditions within-subject, meaningthat participant (identified variable id) tested conditions.dependent variable categorical 5 levels - plot number responses per category two conditions. interested testing whether distribution differ across conditions.use clmm() function package ordinal estimate model. syntax similar use linear mixed effect model. Note output Threshold coefficients latent cutpoints \\(c_1, \\ldots, c_{4}\\)function can -box calculate predictions model us, need coding. also use library DescTools calculate simultaneous multinomial confidence intervals. resulting plot black line model fit, bar observed responses.can repeat process also calculating predictions individual participants","code":"\nlibrary(ggplot2)\nlibrary(ordinal)\nlibrary(tidyverse)\nlibrary(DescTools)\nordered_logistic <- function(eta, cutpoints){\n  cutpoints <- c(cutpoints, Inf)\n  k <- length(cutpoints)\n  p <- rep(NA, k)\n  p[1] <- plogis(cutpoints[1], location=eta, scale=1, lower.tail=TRUE)\n  for(i in 2:k){\n    p[i] <- plogis(cutpoints[i], location=eta, scale=1, lower.tail=TRUE) - \n      plogis(cutpoints[i-1], location=eta, scale=1, lower.tail=TRUE)\n  }\n  return(p)\n}\nset.seed(5)\nN <- 200\nN_id <- 10\ndat <- data.frame(\n  id = factor(sample(1:N_id,N, replace = T)),\n  d1 = rbinom(N,1,0.5), # dummy variable (0,1) indicate 2 conditions\n  x1 = runif(n = N, min = 1, max = 10)\n)\nrfx <- rnorm(length(unique(dat$id)), mean=0, sd=5)\nLP <- 0.5*dat$x1 + 2*dat$d1 + rfx[dat$id]\nfor(i in 1:N){\n  dat$response[i] <- which(rmultinom(1,1, ordered_logistic(LP[i], c(0,2.5, 5,10)))==1)\n}\ndat$response <- factor(dat$response)\nstr(dat)\n#> 'data.frame':    200 obs. of  4 variables:\n#>  $ id      : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 2 9 9 9 5 7 7 3 3 6 ...\n#>  $ d1      : int  0 0 0 0 0 0 0 1 0 0 ...\n#>  $ x1      : num  6.81 1.49 6.94 3.09 3.97 ...\n#>  $ response: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 1 3 2 2 3 3 4 3 2 ...\nggplot(dat,aes(x=response))+\n  geom_bar()+\n  facet_grid(.~d1)\nmodel <- clmm(response ~ x1 + d1 + (1|id), data = dat)\nsummary(model)\n#> Cumulative Link Mixed Model fitted with the Laplace approximation\n#> \n#> formula: response ~ x1 + d1 + (1 | id)\n#> data:    dat\n#> \n#>  link  threshold nobs logLik  AIC    niter    max.grad\n#>  logit flexible  200  -175.75 365.49 291(919) 6.57e-05\n#>  cond.H \n#>  8.6e+02\n#> \n#> Random effects:\n#>  Groups Name        Variance Std.Dev.\n#>  id     (Intercept) 4.224    2.055   \n#> Number of groups:  id 10 \n#> \n#> Coefficients:\n#>    Estimate Std. Error z value Pr(>|z|)    \n#> x1  0.55685    0.07516   7.409 1.27e-13 ***\n#> d1  2.29521    0.36658   6.261 3.82e-10 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Threshold coefficients:\n#>     Estimate Std. Error z value\n#> 1|2  -2.1850     0.8512  -2.567\n#> 2|3   0.2853     0.7664   0.372\n#> 3|4   2.9453     0.8059   3.655\n#> 4|5   7.8126     1.0072   7.757\n# pre-allocate a matrix to store model predictions\n# note that these are a vector of 5 probabilities for each trial\npred_mat <- matrix(NA, nrow=N, ncol=length(unique( dat$response)))\n\nfor(i in 1:N){\n  \n  # first calculate the linear predictor \n  # by summing all variable as indicated\n  # in the model formulate, weighted by the coefficients\n  eta <- dat$x1[i]*model$beta['x1'] +  dat$d1[i]*model$beta['d1'] + model$ranef[dat$id[i]]\n  # note that + model$ranef[dat$id[i]] adds \n  # the random intercept for the subjects of observation i\n  \n  # calculate vector of predicted probabilities\n  pred_mat[i,] <- ordered_logistic(eta, model$Theta)\n  \n}\n\n# add predictions to dataset\npred_dat <- data.frame(pred_mat)\ncolnames(pred_dat) <- paste(\"resp_\",1:ncol(pred_mat),sep=\"\")\npred_dat <- cbind(dat, pred_dat)\n\n# in order to visalize the predictions, \n# we first average them for each condition\npred_dat %>%\n  pivot_longer(cols=starts_with(\"resp_\"), \n               names_prefix=\"resp_\",\n               values_to = \"prob\",\n               names_to =\"response_category\") %>%\n  group_by(d1, response_category) %>%\n  summarise(prob = mean(prob),\n            n=sum(response==response_category)) %>%\n  group_by(d1) %>%\n  mutate(prop_obs = n/sum(n),\n    response=as.numeric(response_category)) -> pred_d1\n#> `summarise()` has grouped output by 'd1'. You can override\n#> using the `.groups` argument.\n\n# cimpute the multinomial interval\npred_d1$CI_lb <- MultinomCI(pred_d1$n)[,\"lwr.ci\"] *2 \npred_d1$CI_ub <- MultinomCI(pred_d1$n)[,\"upr.ci\"] *2\n# note that I multiply for 2 because in the plot each condition \n# will be in a different panel and the probability will sum to 1 in each panel\n\n# visualize (aggregated) ordinal response & prediction\n# the black line are the predictions of the model\n\nggplot(pred_d1,aes(x=response, y=prop_obs))+\n  geom_col()+\n  geom_errorbar(data=pred_d1, aes(ymin=CI_lb, ymax=CI_ub),width=0.2)+\n  facet_grid(.~d1)+\n  geom_line(data=pred_d1, aes(y=prob), size=2)+\n  labs(y=\"probability\")\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2\n#> 3.4.0.\n#> ℹ Please use `linewidth` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where\n#> this warning was generated.\n# split by ID\npred_dat %>%\n  pivot_longer(cols=starts_with(\"resp_\"), \n               names_prefix=\"resp_\",\n               values_to = \"prob\",\n               names_to =\"response_category\") %>%\n  group_by(id, d1, response_category) %>%\n  summarise(prob = mean(prob),\n            n=sum(response==response_category)) %>%\n  group_by(d1, id) %>%\n  mutate(prop_obs = n/sum(n),\n         response=as.numeric(response_category))  -> pred_d1\n#> `summarise()` has grouped output by 'id', 'd1'. You can\n#> override using the `.groups` argument.\n\n# calculate multinomial CI\n# we do a loop over all participants and conditions\npred_d1$CI_lb <- NA\npred_d1$CI_ub <- NA\nfor(i in unique(pred_d1$id)){\n  for(cond in c(0,1)){\n    pred_d1$CI_lb[pred_d1$id==i & pred_d1$d1==cond] <- DescTools::MultinomCI(pred_d1$n[pred_d1$id==i & pred_d1$d1==cond])[,\"lwr.ci\"] \n    pred_d1$CI_ub[pred_d1$id==i & pred_d1$d1==cond] <- DescTools::MultinomCI(pred_d1$n[pred_d1$id==i & pred_d1$d1==cond])[,\"upr.ci\"]\n  }\n}\n\npred_d1 %>%\n  mutate(condition = factor(d1)) %>%\n  ggplot(aes(x=response, y=prop_obs, fill=condition))+\n    geom_col()+\n    geom_errorbar(aes(ymin=CI_lb, ymax=CI_ub, color=condition), width=0.2)+\n    facet_grid(id~d1)+\n    geom_line(aes(y=prob), size=2)+\n    labs(y=\"probability\")"},{"path":"meta-analysis.html","id":"meta-analysis","chapter":" 8 Meta-analyses","heading":" 8 Meta-analyses","text":"running meta-analyses, recommend metafor package R (see link 1, link 2).comprehensive, hands-guide use package provided book Harrer colleagues,19 freely available link.alternative metafor package Bayesian multilevel modelling (also discussed book Harrer colleagues). technical discussion Bayesian multilevel modelling meta-analyes provided paper Williams, Rast Bürkner.20Note: slides workshop meta-analyses using metafor package included workshop section website 11","code":""},{"path":"missing-data.html","id":"missing-data","chapter":" 9 Missing data","heading":" 9 Missing data","text":"","code":""},{"path":"missing-data.html","id":"types-of-missing-data","chapter":" 9 Missing data","heading":"9.1 Types of missing data","text":"Following work Rubin21, missing data typically grouped 3 categories:Missing completely random (MCAR). assumes probability missing cases; implies mechanisms causes missingness related way data. example, say, ’s known unpredictable error server side prevented recording responses respondents survey. missingness entirely independent respondents’ characteristics, MCAR. data MCAR can ignore lot complexities just complete-case analysis (, simply exclude incomplete observations dataset). part possible loss information, complete case analysis introduce bias results. practice, however, difficult establish whether data truly MCAR. Ideally, argue data MCAR, one good idea mechanisms caused missigness (). Formally, data MCAR \\[\n\\Pr(R=0|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi) = \\Pr(R=0|\\psi)\n\\] \\(R\\) indicator variable set 0 missing data 1 otherwise; \\(Y_\\mathrm{obs},Y_\\mathrm{mis}\\) indicate observed missing data, respectively; \\(\\psi\\) simply parameter determine overall (fixed) probability missing.Missing completely random (MCAR). assumes probability missing cases; implies mechanisms causes missingness related way data. example, say, ’s known unpredictable error server side prevented recording responses respondents survey. missingness entirely independent respondents’ characteristics, MCAR. data MCAR can ignore lot complexities just complete-case analysis (, simply exclude incomplete observations dataset). part possible loss information, complete case analysis introduce bias results. practice, however, difficult establish whether data truly MCAR. Ideally, argue data MCAR, one good idea mechanisms caused missigness (). Formally, data MCAR \\[\n\\Pr(R=0|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi) = \\Pr(R=0|\\psi)\n\\] \\(R\\) indicator variable set 0 missing data 1 otherwise; \\(Y_\\mathrm{obs},Y_\\mathrm{mis}\\) indicate observed missing data, respectively; \\(\\psi\\) simply parameter determine overall (fixed) probability missing.Missing random (MAR). less strong assumption missingness systematically related observed unobserved data. example, data MAR study male respondents less likely complete survey depression severity female respondents - , probability reaching end survey related sex (fully observed) severity symptoms. Formally, data MAR \\[\n\\Pr(R=0|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi) = \\Pr(R=0|Y_\\mathrm{obs},\\psi)\n\\] data missing random (MAR) results complete case analyses may biased common approach deal use imputation. Stef van Buuren freely available online book topic22. Among things, illustrates multiple imputation R examples.Missing random (MAR). less strong assumption missingness systematically related observed unobserved data. example, data MAR study male respondents less likely complete survey depression severity female respondents - , probability reaching end survey related sex (fully observed) severity symptoms. Formally, data MAR \\[\n\\Pr(R=0|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi) = \\Pr(R=0|Y_\\mathrm{obs},\\psi)\n\\] data missing random (MAR) results complete case analyses may biased common approach deal use imputation. Stef van Buuren freely available online book topic22. Among things, illustrates multiple imputation R examples.Missing random (MNAR). means probability missing varies reasons unknown us, may depends missing values . Formally means \\(\\Pr(R=0|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi)\\) simplify way. case hard handle: complete case analyses may may biased, way knowing may find information caused missingness.Missing random (MNAR). means probability missing varies reasons unknown us, may depends missing values . Formally means \\(\\Pr(R=0|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi)\\) simplify way. case hard handle: complete case analyses may may biased, way knowing may find information caused missingness.","code":""},{"path":"missing-data.html","id":"deciding-whether-the-data-are-mcar","chapter":" 9 Missing data","heading":"9.2 Deciding whether the data are MCAR","text":"MCAR scenario safe complete case analysis, seem useful way test assumption. approaches proposed test whether data MCAR, widely used ’s clear useful practice. example one run logistic regression “missingness” dependent variable (e.g. indicator variable set 1 data missing 0 otherwise), variables predictors - data MCAR none predictors predict missingness. popular alternative, implemented several software packages Little’s test23.Technically, approaches can help determine whether missingness depends observed variables (, MAR), strictly speaking exclude missingness due unobserved variables (MNAR scenario). Nevertheless, one good reasons believe data MCAR, want add statistical test corroborate assumption, reasonable tests . However, remains important also discuss openly possible reasons mechanisms missingness, explain deem priori plausible data MCAR. fact, statistical tests alone tell whether data missing completely random. terms MCAR, MAR MNAR refers causal mechanisms responsible missing data , strictly speaking, causal claims decided uniquely basis simple statistical test. data “pass” test provide additional support assumption MCAR, test alone fully satisfy assumptions MCAR. see note MCAR (formally defined ) assumes also relationship missingness particular variable values variable: since question missing data, tested quantitative analysis available data. Finally, added null-hypothesis significance test, failure reject null hypothesis , , provide evidence null hypothesis (data MCAR). may also don’t enough power reliably detect pattern missingness.","code":""},{"path":"missing-data.html","id":"causal-analysis-and-bayesian-imputation","chapter":" 9 Missing data","heading":"9.3 Causal analysis and Bayesian imputation","text":"best principled approach deal missingness (least opinion) think hard causal mechanisms may determine missingness, use assumption causal mechanisms perform full Bayesian imputation (, treating missing data parameter estimating ).plan create include worked example ; meantime interested readers referred Chapter 15 (particular section 15.2) excellent book Richard McElreath Statistical Rethinking24 present accessible worked example R.","code":""},{"path":"SDT.html","id":"SDT","chapter":" 10 Signal Detection Theory","heading":" 10 Signal Detection Theory","text":"Signal Detection Theory (hereafter abbreviated SDT) probably important influential framework modelling perceptual decisions forced-choice tasks, wide applicability also beyond purely perceptual decision tasks. review derive fundamental concepts equal-variance unequal variance signal detection theory, present R code simulate confidence optimal observer/decision-maker.","code":""},{"path":"SDT.html","id":"equal-variance-gaussian-sdt","chapter":" 10 Signal Detection Theory","heading":"10.1 Equal-variance Gaussian SDT","text":"SDT relies idea information available observer / decision-maker can modeled single random variable latent decision space. SDT typically () applied detection tasks: trials stimulus presented, consisting background noise without signal. observer tasked deciding whether signal present absent (thus 2 possible responses, yes ). SDT assumes trial observer makes decision basis random variable, call \\(X\\), may drawn signal distribution noise distribution.simplest case, signal distribution Gaussian variance \\(\\sigma^2=1\\) mean \\(d'>0\\).\\[\n\\begin{aligned}\nf_S(x)&=\\frac{1}{\\sigma\\sqrt{2 \\pi}} e^{-\\frac{(x-d')^2}{2 \\sigma^2}}\\\\\n&=\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{(x-d')^2}{2}}\n\\end{aligned}\n\\]noise distribution second normal distribution mean \\(0\\) variance \\(\\sigma^2=1\\)\\[\nf_N(x)=\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{x^2}{2}}\n\\]Note prior probability signal noise may equal. Let’s define probability signal-present trial \\(p(S)=\\alpha\\); thus \\(p(N)=1-p(S)=1-\\alpha\\).","code":""},{"path":"SDT.html","id":"optimal-decision-rule","chapter":" 10 Signal Detection Theory","heading":"10.1.1 Optimal decision rule","text":"optimal way decide whether particular value \\(x\\) drawn signal noise distribution using likelihood-ratio, one responde “yes” whenever\\[\n\\begin{aligned}\n\\frac{f_S(x)\\,p(S)}{f_N(x) \\, p(N)} & \\ge 1 \\\\\n\\frac{f_S(x)\\,\\alpha}{f_N(x) \\, (1-\\alpha)} & \\ge 1\n\\end{aligned}\n\\]algebraic manipulations, can shown likelihood ratio decision rule amounts comparing value \\(x\\) criterion \\(c\\):\\[\n\\begin{aligned}\n\\frac{f_S(x)\\alpha}{f_N(x)(1-\\alpha)} & \\ge 1 \\\\\n\\frac{f_S(x)}{f_N(x)} & \\ge \\frac{1-\\alpha}{\\alpha} \\\\\n\\frac{\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{(x-d')^2}{2}}}{\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{x^2}{2}}} & \\ge \\frac{1-\\alpha}{\\alpha} \\\\\n\\frac{e^{-\\frac{(x-d')^2}{2}}}{e^{-\\frac{x^2}{2}}} & \\ge \\frac{1-\\alpha}{\\alpha}\n\\end{aligned}\n\\]taking log sides\\[\n\\begin{aligned}\n\\log \\left(\\frac{e^{-\\frac{(x-d')^2}{2}}}{e^{-\\frac{x^2}{2}}} \\right) & \\ge \\log \\left(\\frac{1-\\alpha}{\\alpha}\\right) \\\\\n  \\log \\left(e^{-\\frac{(x-d')^2}{2}} \\right) - \\log \\left(e^{-\\frac{x^2}{2}} \\right) & \\ge \\log \\left(\\frac{1-\\alpha}{\\alpha}\\right) \\\\\n-\\frac{(x-d')^2}{2} + \\frac{x^2}{2}  & \\ge \\log \\left(\\frac{1-\\alpha}{\\alpha}\\right) \\\\\n\\frac{-x^2 -(d')^2 + 2d' x + x^2 }{2} & \\ge \\log \\left(\\frac{1-\\alpha}{\\alpha}\\right) \\\\\n\\frac{-(d')^2 + 2d'}{2}x & \\ge \\log \\left(\\frac{1-\\alpha}{\\alpha}\\right) \\\\\nd'x -\\frac{(d')^2}{2}& \\ge \\log \\left(\\frac{1-\\alpha}{\\alpha}\\right) \\\\\nd'x & \\ge \\log \\left(\\frac{1-\\alpha}{\\alpha}\\right) + \\frac{(d')^2}{2}\\\\\nx & \\ge \\frac{1}{d'}\\log \\left(\\frac{1-\\alpha}{\\alpha}\\right)+ \\frac{d'}{2}\n\\end{aligned}\n\\]optimal criterion thus found \\(c = \\frac{1}{d'}\\log \\left(\\frac{1-\\alpha}{\\alpha}\\right)+ \\frac{d'}{2}\\). Whenever \\(x\\) exceed criterion, \\(x \\ge c\\), observer respond “signal present” respond “signal absent” otherwise.easy verify signal noise trials equiprobable, \\(p(s) = p(N) \\implies \\alpha=0.5\\), optimal criterion becomes \\(c=\\frac{d'}{2}\\).","code":""},{"path":"SDT.html","id":"visualizing-signal-detection-theory-in-r","chapter":" 10 Signal Detection Theory","heading":"10.1.1.1 Visualizing signal detection theory in R","text":"use R verify visually optimal criterion (represented vertical red line) located horizontal coordinates crossover point two probability densities:(Note noise distribution, dark grey, centered zero. signal distribution centered \\(d'\\).)unequal prior probabilities, e.g. \\(p(S)=\\alpha=0.8\\) ?optimal criterion always crossover point, however different location since two distribution scaled prior proability.","code":"\n# settings\nd_prime <- 2\nsigma <- 1   \nalpha <- 0.5\n\n# support of random variable X (for plotting)\nsupp_x <- seq(-2,4,length.out=500) \n\n# calculate optima criterion\noptimal_c <- 1/d_prime * log((1-alpha)/alpha) + d_prime/2\n\n# calculate probability density and scale by prior probability\nfS <- alpha*dnorm(supp_x, mean=d_prime, sd=sigma)\nfN <- (1-alpha)*dnorm(supp_x, mean=0, sd=sigma)\n\n# plot \nplot(supp_x, fS, type=\"l\",lwd=2,col=\"black\",xlab=\"X\",ylab=\"p(X)\")\nlines(supp_x, fN, lwd=2,col=\"dark grey\")\nabline(v=optimal_c,lwd=1.5,lty=1,col=\"red\")\nlegend(\"topleft\",c(expression(\"f\"[\"S\"]),expression(\"f\"[\"N\"])),col=c(\"black\",\"dark grey\"),lwd=2,title = \"source:\",bty=\"n\")\n# Set a different prior probability\nalpha <- 0.8\n\n# calculate optima criterion\noptimal_c <- 1/d_prime * log((1-alpha)/alpha) + d_prime/2\n\n# calculate probability density and scale by prior probability\nfS <- alpha*dnorm(supp_x, mean=d_prime, sd=sigma)\nfN <- (1-alpha)*dnorm(supp_x, mean=0, sd=sigma)\n\n# plot \nplot(supp_x, fS, type=\"l\",lwd=2,col=\"black\",xlab=\"X\",ylab=\"p(X)\")\nlines(supp_x, fN, lwd=2,col=\"dark grey\")\nabline(v=optimal_c,lwd=1.5,lty=1,col=\"red\")\nlegend(\"topleft\",c(expression(\"f\"[\"S\"]),expression(\"f\"[\"N\"])),col=c(\"black\",\"dark grey\"),lwd=2,title = \"source:\",bty=\"n\")"},{"path":"SDT.html","id":"estimating-the-parameters-from-data","chapter":" 10 Signal Detection Theory","heading":"10.1.2 Estimating the parameters from data","text":"parameters can easily estimated form proportions hits, \\(p_\\text{H}\\), false alarms \\(p_{\\text{FA}}\\). (Hits correct ‘yes’ responses faalse alarms incorrect ‘yes’ responses.)Consider first probability false alarm just probability observing \\(X \\ge c\\) \\(X\\) drawn noise distribution \\(f_N\\).Thus\\[\np_{\\text{FA}} =  1 - \\Phi(c)\n\\]\n\\(\\Phi\\) cumulative distribution function standard normal distribution words, \\(\\Phi(c)\\) area lies left \\(c\\) Gaussian function mean \\(0\\) variance \\(1\\).Similarly \\(X\\) drawn signal distribution, probability hit response probability \\(x\\) drawn signal distribution \\(f_S\\) greater criterion \\(c\\),\n\\[\np_{\\text{H}} =  1 - \\Phi(c - d')\n\\]\nThus, know proportion hits false alarms, can estimate \\(c\\) \\(d'\\) using inverse \\(\\Phi\\), can notated \\(\\Phi^{-1}\\) often referred quantile function\\[\nc = \\Phi^{-1}\\left(1 -  p_{\\text{FA}}\\right) = - \\Phi^{-1}\\left(p_{\\text{FA}}\\right) \\\\\nd' = c - \\Phi^{-1}\\left(1 -  p_{\\text{H}}\\right) =\\Phi^{-1}\\left(p_{\\text{H}}\\right) - \\Phi^{-1}\\left(p_{\\text{FA}}\\right)\n\\]","code":""},{"path":"SDT.html","id":"glm-formulation-of-equal-variance-sdt-models","chapter":" 10 Signal Detection Theory","heading":"10.1.3 GLM formulation of equal-variance SDT models","text":"Note SDT model reformulated probit generalized linear model. expressed \\[\n\\Phi^{-1}(p_{\\text{yes}}) = \\beta_0 + \\beta_1 x\n\\]\n\\(p_{\\text{yes}}\\) probability observer responding signal present, \\(x\\) variable indicates presence/absence signal 1/0, respectively. similarity SDT model evident consider , GLM, probability hit false alarm correspond \\(p_{\\text{yes}}\\) signal present (\\(x=1\\)) absent (\\(x=0\\)), respectively. allows mapping signal detection theory parameters, \\(d'\\) \\(c\\) GLM intercept slope parameters, \\(\\beta_0\\) \\(\\beta_1\\)\\[\nc = -\\Phi^{-1}(p_{\\text{FA}}) = - \\beta_0\n\\]\n\\[\nd' = \\Phi^{-1}(p_{\\text{H}}) - \\Phi^{-1}(p_{\\text{FA}}) = \\beta_0 + \\beta_1 -\\beta_0 = \\beta_1\n\\]\nRecognizing identity makes easier use statistical packages R easily analyse complex design multiple conditions interaction effects. also makes possible estimate multi-level (hierarchical, random-effects) SDT models.","code":""},{"path":"SDT.html","id":"bayesian-confidence-in-equal-variance-sdt","chapter":" 10 Signal Detection Theory","heading":"10.2 Bayesian confidence in equal-variance SDT","text":"Formally, confidence Bayesian (, subjective) posterior probability decision correct given evidence available observer.First notation. Let’s use \\(S\\) indicate event signal present (event something can assign probability) \\(N\\) indicate event noise presented. Say observer respond yes, confidence correspond posterior probability \\(p(S\\mid x)\\), probability signal present given observed \\(x\\).can calculated applying Bayes theorem:\\[\n\\begin{aligned}\np(S \\mid x) & = \\frac{p(x \\mid S)\\times p(S)}{p(x)} \\\\\n& = \\frac{p(x \\mid S) p(S)}{p(x \\mid S) p(S)+p(x \\mid N) p(N)} \\\\\n& = \\frac{p(x \\mid S) \\alpha}{p(x \\mid S) \\alpha+p(x \\mid N) (1-\\alpha)}\n\\end{aligned}\n\\]can simplified cases equal probabilities \\(p(s) = p(N) \\implies \\alpha=0.5\\)\n\\[\np(S \\mid x)_{\\alpha=0.5} =  \\frac{p(x \\mid S)}{p(x \\mid S)+p(x \\mid N)}\n\\]Note also confidence signal absent trials \\(N\\) calculated way:\\[\np(N \\mid x)_{\\alpha=0.5} =  \\frac{p(x \\mid S)}{p(x \\mid N)+p(x \\mid N)}\n\\]One question may ask point distribution confidence levels observers changes correct vs wrong responses, also signal absent vs signal present responses. simplest way get simulation - see following R codeThe distribution confidence - expected - different correct wrong response: peaked near 1 correct responses, peaked near 0.5 errors. Importantly, separation confidence distributions correct wrong responses similar signal absent (left panel) signal present (right panel) trials. suggest metacognitive sensitivity - ability discriminate correct incorrect responses - change across signal present signal absent answers. can visualized Type-2 ROC (Receiver Operating Characteristic) curves, obtained plotting proportion “type-2 hits” function “type-2 false alarms” - fraction correct wrong responses classified correct possible\nthreshold setting confidence distribution. term “Type-2” used indicate metacognitive task - decision decision25.ROC curve derived confidence ratings discriminate equally well correct vs wrong responses (plot, blue curve target present responses) - thus indicating similar metacognitive “type-2” sensitivity. However, empiricaly found metacognitive sensitivite seems worse signal absent responses - e.g. see.26 see , finding can accomodated relaxing assumption signal noise distribution standard deviation, assuming instead standard deviation signal present distribution larger.","code":"\n# load ggplot library for plotting\nlibrary(ggplot2)\n\n# settings\nd_prime <- 1.5\nsigma <- 1   \nalpha <- 0.5\n\n# calculate optimal criterion\noptimal_c <- 1/d_prime * log((1-alpha)/alpha) + d_prime/2\n\n# simulate 2*10^4 trials and calculate the confidence \nN_trials <- 2*10^3\ntar_pres <- c(rep(0,N_trials/2),rep(1,N_trials/2))\n\n# simulate X by adding Gaussian noise (function rnorm())\nx <- tar_pres*d_prime + rnorm(length(tar_pres), mean=0, sd=1)\nresp_yes <- ifelse(x >= optimal_c,1,0)\n\n# define a custom function to calculate confidence\nconfidenceSDT1 <- function(x,resp,d_prime, alpha=0.5){\n  conf <- ifelse(\n    resp==1,\n    dnorm(x,mean=d_prime,sd=1)/(dnorm(x,mean=d_prime,sd=1)+dnorm(x,mean=0,sd=1)),\n    dnorm(x,mean=0,sd=1)/(dnorm(x,mean=d_prime,sd=1)+dnorm(x,mean=0,sd=1))\n  )\n  return(conf)\n}\n\n# calculate confidence\nconfidence <- confidenceSDT1(x, resp_yes, d_prime=1.5)\n\n# put into a dataframe for plotting\nd <- data.frame(confidence, x, tar_pres, resp_yes)\n\n# check which simulated responses are correct\nd$correct <- ifelse((d$tar_pres==1 & d$resp_yes==1)|(d$tar_pres==0 & d$resp_yes==0),1,0)\n\n# plot\nd$tar <- ifelse(d$tar_pres==1,\"signal present\",\"signal absent\")\nd$correct <- ifelse(d$correct==1,\" correct response\",\"wrong response\")\nggplot(d,aes(x=confidence,group=correct,color=correct,fill=correct))+\n  geom_histogram(position = 'dodge',aes(y=..density..), binwidth=0.05,alpha=0.9)+\n  facet_grid(.~tar)+\n  scale_color_manual(values=c(\"dark green\",\"red\"),name=\"\")+\n  scale_fill_manual(values=c(\"dark green\",\"red\"),name=\"\")+\n  labs(x=\"confidence level\")+\n  theme_classic()\n#> Warning: The dot-dot notation (`..density..`) was deprecated in\n#> ggplot2 3.4.0.\n#> ℹ Please use `after_stat(density)` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where\n#> this warning was generated.\n# functions to compute true and false positive rates\nTPR <- function(d,th){ sum(d$tar_pres==1 & d$x>th) / sum(d$tar_pres==1)}\nFPR <- function(d,th){ sum(d$tar_pres==0 & d$x>th) / sum(d$tar_pres==0)}\n\n# use all the sorted values are possible threshods\nthresholds <- sort(d$x)\n\nroc <- data.frame(y=sapply(thresholds, function(th){TPR(d[d$resp_yes==1,],th)}), \n                 x=sapply(thresholds, function(th){FPR(d[d$resp_yes==1,],th)}) )\n\nroc0 <- data.frame(y=sapply(thresholds, function(th){TPR(d[d$resp_yes==0,],th)}), \n                 x=sapply(thresholds, function(th){FPR(d[d$resp_yes==0,],th)}) )\n\nggplot(roc,aes(x,y))+geom_point(color=\"blue\")+theme_classic()+labs(y=\"Type-2 hits\", x=\"Type-2 FA\") +geom_abline(intercept=0,slope=1,lty=2)+geom_point(data=roc0,color=\"dark red\")+ggtitle(\"Equal-variance SDT, Type 2 sensitivity\")"},{"path":"SDT.html","id":"unequal-variance-sdt","chapter":" 10 Signal Detection Theory","heading":"10.3 Unequal-variance SDT","text":"Signal detection theory can extended account cases signal noise distribution different variance. many types random processes, mean variance related signals higher mean also higher variance (firing rate neurons example). Thus signal distribution now given \\[\nf_S(x)=\\frac{1}{\\sigma\\sqrt{2 \\pi}} e^{-\\frac{(x-d')^2}{2 \\sigma^2}}\n\\]variance \\(\\sigma^2 \\ne 1\\).One important consequence different noise level, now log-likelihood ratio quadratic function signal x\\[\n\\log \\left( \\frac{f_S(x)}{f_N(x)} \\right) = -\\frac{1}{2 \\sigma^2} \\left[\\left(1 - \\sigma^2 \\right)x^2 -2d' x + d'^2 + 2\\sigma^2 \\log \\sigma\\right]\n\\]\nresult, log-likelihood ratio crosses zero 2 points, thus yielding 2 decision criteria - see next figure.reason two criteria may seen clearly plot logarithm probability density, makes evident two regions, left right noise distribution, probability signal present larger -signal (.e noise ).","code":"\n# settings\nd_prime <- 3\nsigmaS <- 2  \nsigmaN <- 1 # fixed\nalpha <- 0.5\n\n\n# calculate optimal UEV-SDT criterion\nUVGSDTcrit <- function(dp, sig, logbeta=0){\n  TwoSigSq <- 2 * sig^2\n  \n  minLam <- optimize(function(X, dp, sig){-((1 - sig^2) * X^2 - 2 * dp * X + dp^2 + TwoSigSq * log(sig))/TwoSigSq}, c(-10, 10), dp = dp, sig = sig)$objective\n  \n  if(logbeta < minLam){ warning(\"complex roots\")}\n  \n  cf <- -c(dp^2 + TwoSigSq * log(sig) + logbeta * TwoSigSq,-2 * dp, 1 - sig^2)/TwoSigSq\n  \n  proot <- polyroot(cf)\n  \n  return(sort(Re(proot)))\n}\n\n#\nUE_c <- UVGSDTcrit(d_prime, sigmaS)\n\n# simulate 2*10^3 trials and calculate the confidence \nN_trials <- 2*10^3\ntar_pres <- c(rep(0,N_trials/2),rep(1,N_trials/2))\n\n# simulate X by adding Gaussian noise (function rnorm())\n# first generate internal responses\nx <- rep(NA,length(tar_pres))\nx[tar_pres==0] <- rnorm(N_trials/2, mean=0,sd=1)\nx[tar_pres==1] <- rnorm(N_trials/2, mean=d_prime,sd=sigmaS)\n\n\n# plot unequal variance and criterion\nXi <- seq(-6,10,length.out=500)\nfS = dnorm(Xi,mean=d_prime,sd=sigmaS)\nfN = dnorm(Xi,mean=0,sd=1)\nplot(Xi,fN,type=\"l\",col=\"grey\",lwd=3,ylab=\"density\",xlab=\"X\")\nlines(Xi,fS,lwd=3)\nabline(v=UE_c,col=\"red\",lwd=2)\n# plot unequal variance and criterion\nXi <- seq(-6,10,length.out=500)\nfS = dnorm(Xi,mean=d_prime,sd=sigmaS)\nfN = dnorm(Xi,mean=0,sd=1)\nplot(Xi,fN,type=\"l\",col=\"grey\",lwd=3,ylab=\"density\",xlab=\"X\", log=\"y\")\nlines(Xi,fS,lwd=3)\nabline(v=UE_c,col=\"red\",lwd=2)"},{"path":"SDT.html","id":"optimal-confidence-in-unequal-variance-sdt","chapter":" 10 Signal Detection Theory","heading":"10.3.1 Optimal confidence in unequal-variance SDT","text":"confidence can computed way (applying Bayes rule).can seen ROC curve, confidence levels (even estimated optimally using Bayes rule) reveals asymmetry (, target present responses represented blue curve). , unequal-variance signal detection theory model predict worse metacognitive sensitivity “signal absent” responses.","code":"\n# now apply decision rule\nif(length(UE_c)==1){\n  resp_yes <- ifelse(x > UE_c,1,0)\n}else{\n  resp_yes <- ifelse((x <= UE_c[1])|(x>UE_c[2]),1,0)\n}\n\n\n# define a custom function to calculate confidence\nconfidenceSDT1 <- function(x,resp,dp=d_prime, alpha=0.5){\n  conf <- ifelse(\n    resp==1,\n    dnorm(x,mean=dp,sd=sigmaS)/(dnorm(x,mean=dp,sd=sigmaS)+dnorm(x,mean=0,sd=1)),\n    dnorm(x,mean=0,sd=1)/(dnorm(x,mean=dp,sd=sigmaS)+dnorm(x,mean=0,sd=1))\n  )\n  return(conf)\n}\n\n# calculate confidence\nconfidence <- confidenceSDT1(x, resp_yes)\n\n# put into a dataframe for plotting\nd <- data.frame(confidence, x, tar_pres, resp_yes)\n\n# check which simulated responses are correct\nd$correct <- ifelse((d$tar_pres==1 & d$resp_yes==1)|(d$tar_pres==0 & d$resp_yes==0),1,0)\n\n# plot\nd$tar <- ifelse(d$tar_pres==1,\"signal present\",\"signal absent\")\nd$correct <- ifelse(d$correct==1,\" correct response\",\"wrong response\")\nggplot(d,aes(x=confidence,group=correct,color=correct,fill=correct))+\n  geom_histogram(position = 'dodge',aes(y=..density..), binwidth=0.05,alpha=0.9)+\n  facet_grid(.~tar)+\n  scale_color_manual(values=c(\"dark green\",\"red\"),name=\"\")+\n  scale_fill_manual(values=c(\"dark green\",\"red\"),name=\"\")+\n  labs(x=\"confidence level\")+\n  theme_classic()+\n  ggtitle(\"Unequal-variance SDT\")\n# functions to compute true and false positive rates\nTPR <- function(d,th){ sum(d$tar_pres==1 & d$x>th) / sum(d$tar_pres==1)}\nFPR <- function(d,th){ sum(d$tar_pres==0 & d$x>th) / sum(d$tar_pres==0)}\n\n# use all the sorted values are possible threshods\nthresholds <- sort(d$x)\n\nroc <- data.frame(y=sapply(thresholds, function(th){TPR(d[d$resp_yes==1,],th)}), \n                 x=sapply(thresholds, function(th){FPR(d[d$resp_yes==1,],th)}) )\n\nroc0 <- data.frame(y=sapply(thresholds, function(th){TPR(d[d$resp_yes==0,],th)}), \n                 x=sapply(thresholds, function(th){FPR(d[d$resp_yes==0,],th)}) )\n\nggplot(roc,aes(x,y))+geom_point(color=\"blue\")+theme_classic()+labs(y=\"Type-2 hits\", x=\"Type-2 FA\") +geom_abline(intercept=0,slope=1,lty=2)+geom_point(data=roc0,color=\"dark red\")+ggtitle(\"Unequal-variance SDT, Type 2 sensitivity\")"},{"path":"workshops.html","id":"workshops","chapter":" 11 Workshops","heading":" 11 Workshops","text":"Either click links click embedded slides press ‘F’ go full screen. Advance slides arrow keys, press ‘O’ overview slides.","code":""},{"path":"workshops.html","id":"linear-multilevel-models-lmm-workshop-9th-sept-2022","chapter":" 11 Workshops","heading":"11.1 Linear multilevel models (LMM) workshop (9th Sept 2022)","text":"Part 1 (link)Part 2 (link)PracticalsExercises solution (link)","code":""},{"path":"workshops.html","id":"introduction-to-meta-analyses-in-r","chapter":" 11 Workshops","heading":"11.2 Introduction to meta-analyses in R","text":"22nd February 2023(link)","code":""},{"path":"workshops.html","id":"power-analyses-via-data-simulation","chapter":" 11 Workshops","heading":"11.3 Power analyses via data simulation","text":"15th November 2023Links: (slides),\n(script)","code":""},{"path":"workshops.html","id":"introduction-to-bayesian-data-analysis-using-r-and-stan","chapter":" 11 Workshops","heading":"11.4 Introduction to Bayesian data analysis using R and Stan","text":"29th November 2023Links: (slides),See Github repository code data: https://github.com/mattelisi/intro-Bayes.","code":""},{"path":"workshops.html","id":"introduction-to-linear-algebra","chapter":" 11 Workshops","heading":"11.5 Introduction to linear algebra","text":"Lecture MSc module PS5210 (version : 7th December 2022)Part 2 (link)","code":""},{"path":"useful-links-resources.html","id":"useful-links-resources","chapter":" 12 Useful links & resources","heading":" 12 Useful links & resources","text":"","code":""},{"path":"useful-links-resources.html","id":"statistical-theory","chapter":" 12 Useful links & resources","heading":"12.1 Statistical theory","text":"","code":""},{"path":"useful-links-resources.html","id":"map-of-univariate-statistical-distributions","chapter":" 12 Useful links & resources","heading":"12.1.1 Map of univariate statistical distributions","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
