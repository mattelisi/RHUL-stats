--- 
title: "RHUL Psychology\nStatistical modelling notebook"
author: "Matteo Lisi"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
url: https://mlisi.xyz/RHUL-stats/
cover-image: images/pop_brain.png
description: |
  Notebook for stats support activity at RHUL psych department.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---
```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```

# About

![Pop-art brain according to [Dall-E](https://labs.openai.com/s/Zr8sPS7hl9f6hZRKtQhrbTzO).](images/pop_brain.png){width=75%}

This online book is created and maintained by [Matteo Lisi](https://mlisi.xyz/) and is meant to be a shared resource for staff and students at the [Department of Psychology of Royal Holloway, University of London](https://www.royalholloway.ac.uk/research-and-teaching/departments-and-schools/psychology/). It will contain a miscellanous set of tutorial, examples, case studies, workshops materials and any other useful material related to data analysis and modelling. These will be added and revised over time, based on the most common questions and requests that I receive.

$$\\[1in]$$

---

<sub>
This is a work in progress and may contain imprecisions and typos. If you spot any please let me know at matteo.lisi [at] rhul.ac.uk.
The materials that will be included builds upon and draw from existing literature on statistics and modelling. I will endeavor to properly cite existing books and papers; but if any author feels that I have not given them fair acknowledgement, please let me know and I will make amend. This book is licenbsed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license.
</sub>


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Departmental survey about statistical methods {#surveys}

I used an anonymous survey to ask colleagues some questions about which topics may be more interesting or useful in their research. 

## March 2022

### Question 1

In the first question people indicated topics of interests. The winner are multilevel models, followed closely by Bayesian statistics.

```{r, message=FALSE, echo=FALSE, warning=F,fig.height=9,fig.width=7,fig.align='center'}
library(tidyverse)

# load Qualtrics csv output (edited to remove double header and IP addresses)
d <- read_csv('../data/survey_march22/stats-survey-rhul_edit.csv')

d %>%
  filter(Finished==TRUE) %>%
  dplyr::select(starts_with('models_') & !contains('TEXT')) %>%
  mutate(id = 1:nrow(.)) %>%
  pivot_longer(starts_with('models_'),names_to = 'what') %>%
  filter(!is.na(value)) %>%
  filter(value!="Other (please specify)") %>%
  mutate(value_short = str_extract(value, pattern='^[^:\\.)]*')) %>% 
  # simplify labels (could be more elegant if I knew regex better)
  mutate(value_short = ifelse(str_detect(value_short,"\\((?=e)"),
                              str_sub(value_short,1,-1 -2),
                              value_short)) %>%
  mutate(value_short = ifelse(str_detect(value_short,"\\("),
                              str_c(value_short,")"),
                              value_short)) %>%
  group_by(value_short) %>%
  summarise(N = length(id)) %>%
  arrange(N) %>%
  mutate(value_short=as_factor(value_short)) %>%
  ggplot(aes(y=value_short, x=N))+
  theme_gray(12) +
  geom_col() +
  scale_y_discrete(labels = function(x) lapply(strwrap(x, width = 30, simplify = FALSE), paste, collapse="\n")) +
  labs(y="")
```

There were some additional suggestions.

```{r echo=FALSE}
d %>%
  filter(Finished==TRUE) %>%
  select(starts_with('models_') & contains('TEXT')) %>%
  filter(!is.na(models_17_TEXT)) %>%
  pull(models_17_TEXT) 

```

A few other topics were mentioned in the comment section:

- Shiny apps
- Network modelling
- Longitudinal analyses
- Random forests
- Neural network


### Question 2

Here people indicated their interest for topics related to data analysis.

```{r, message=FALSE, echo=FALSE, warning=F,fig.height=4,fig.width=7,fig.align='center'}
d %>%
  filter(Finished==TRUE) %>%
  dplyr::select(starts_with('extra_') & !contains('TEXT')) %>%
  mutate(id = 1:nrow(.)) %>%
  pivot_longer(starts_with('extra_'),names_to = 'what') %>%
  filter(!is.na(value)) %>%
  filter(value!="Other (please specify)") %>%
  group_by(value) %>%
  summarise(N = length(value)) %>%
  arrange(N) %>%
  mutate(value=as_factor(value)) %>%
  ggplot(aes(y=value, x=N))+
  theme_gray(12) +
  geom_col() +
  scale_y_discrete(labels = function(x) lapply(strwrap(x, width = 30, simplify = FALSE), paste, collapse="\n")) +
  labs(y="")
```

Other things mentioned in the comments were:

- SPM
- Docker
- Python


### Question 4

This question was about likelihood of using different formats of support

```{r, message=FALSE, echo=FALSE, warning=F,fig.height=6,fig.width=6,fig.align='center'}
d %>%
  filter(Finished==TRUE) %>%
  dplyr::select(starts_with('format_')) %>%
  mutate(id = 1:nrow(.)) %>%
  pivot_longer(starts_with('format_'),names_to = 'what') %>%
  mutate(value=factor(value, ordered=T, levels=c("Very unlikely",
                                                 "Unlikely",
                                                 "Unsure",
                                                 "Likely",
                                                 "Very likely"))) %>%
  filter(!is.na(value)) %>%
  mutate(what=case_when(
    str_detect(what,"1") ~ "Short workshop",
    str_detect(what,"2") ~ "Longer workshop",
    str_detect(what,"3") ~ "one-on-one meeting",
    str_detect(what,"4") ~ "shared resource",
    str_detect(what,"5") ~ "collaboration"
  ) ) %>%
  group_by(what, value) %>%
  summarise(N = length(id)) %>%
  ggplot(aes(x=value, y=N)) +
  geom_col()+
  facet_wrap(.~what, ncol=2) +
  labs(x="")
```

### Respondents' status

The final questions asked about the status / career level.

```{r, message=FALSE, echo=FALSE, warning=F,fig.height=4,fig.width=6,fig.align='center'}
d %>%
  filter(Finished==TRUE) %>%
  filter(!is.na(role)) %>%
  ggplot(aes(x=role))+
  geom_histogram(stat='count')
```



<!--chapter:end:01-surveys.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Introduction to R {#intro-R}

Most of the practical statistical tutorials and recipes in this book use the software R, so this section provides some introduction to R for the uninitiated. 

---

$$\\[1in]$$

## Installing R

The base R system can be downloaded at the following link, which provides installers for both Windows, Mac and Linux:

[https://cran.rstudio.com/](https://cran.rstudio.com/)

In addition to the base R system, it is useful to have also R-studio, which is an IDE (Integrated Development Environment) for R, and provides both an editor, a graphical interface and much more. It can be downloaded from:

[https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/)


## First steps

R is a programming language and free software environment for statistical computing and graphics. It is an _interpreted language_, which means that to give instructions to the computer you do not have to compile it first in machine language, everything is done 'on the fly' through a command line interpreter, e.g. if you type `2+2` in the command line R, the computer will reply with the answer (try this on your computer):
```{r}
2+2
```

Typically the normal workflow involve writing and saving a series of instructions in a _script_ file (usually saved with the `.R` extension), which can be executed (either step by step or all at once). Since all steps of the analyes are documented in the script, this makes them transparent and reproducible.

In an R script you can use the `#` sign to add comments, so that you and others can understand what the R code is about. Commented lines are ignored by R, so they will not influence your result. See the next example:
```{r}
# calculate 3 + 4
3 + 4
```

### Arithmetic with R
In its most basic form, R can be used as a simple calculator. Consider the following arithmetic operators:

  * Addition: `+`
  * Subtraction: `-`
  * Multiplication: `*`
  * Division: `/`
  * Exponentiation: `^`
  * Modulo: `%%`

The last two might need some explaining:

The ^ operator raises the number to its left to the power of the number to its right: for example `3^2` is 9.

The modulo returns the remainder of the division of the number to the left by the number on its right, for example 5 modulo 3 (or `5 %% 3`) is 2.


### Variable assignment

A basic concept in programming (statistical or not) is called a _variable_.

A variable allows you to store a value (e.g. 2) or an object (e.g. a function description) in R. You can then later use this variable's name to easily access the value or the object that is stored within this variable.

You can assign a value `2` to a variable `my_var` with the command
```{r}
my_var <- 2
```

Note that you would have obtained the same result using:

```{r}
2 -> my_var
```

that is, the _assignment operator_ works in both directions `<-` and `->`.


The variable can then be used in any computation, for example:
```{r}
my_var + 2 
```


### Basic data types in R

Variables can be of many types, not just numerical values. For example, they can contain _text_ values (e.g. a string of characters). Arithmetic operators such as `+` do no work with these. If you tried to apply them characters R will give you an error message.

```{r, error=TRUE}
# Assign a value to the variable apples
apples <- 5 

# Assign a text value
oranges <- "six" 

#  
apples + oranges 
```



In fact R works with numerous data types, and some of these are not numerical (so they can't be added, subtracted, etc.). Some of the most basic types to get started are:

  * Decimal values like 4.5 are called numerics.
  * Natural numbers like 4 are called integers. Integers are also numerics.
  * Boolean values (`TRUE` or `FALSE`, abbreviated `T` and `F`) are called logical^[Note that you can add or multiply logical Boolean values: internally `FALSE` are treated as zeroes, and `TRUE` as ones.].
  * Text (or string) values are called characters.


### Vectors and other data types

Additionally, the simple data types listed above can be combined in more complex 'objects' that can comprise several values. For example, we can obtain a _vector_ by concatenating values using the function `c()`. This can be applied both on numerical or character data types, e.g. 

```{r}
some_numbers <- c(4,87,10, 0.5, -6)
some_numbers

my_modules <- c("PS115", "PS509", "PS300", "PS938", "PS9457")
my_modules
```

There are some special handy functions to create specific types of vectors, such as _sequences_ (using the function `seq()` or the operator `:`)

```{r}
x <- seq(from = -10, to = 10, by = 2)
x

y <- seq(-0, 1, 0.1)
y

z <- 1:5
z
```


Another useful type of vector can be obtained by _repetition_ of elements, and this can be numerical, character, or even applied to other vectors

```{r}
rep(3, 5)

x <- 1:3
rep(x, 4)

rep(c("leo the cat", "daisy the dog"), 2)
```

We can combine vectors of different types into a _data frame_, one of the most useful ways of storing data in R. Let's say we have 3 vectors:

```{r}
# create a numeric vector 
a <- c(0, NA, 2:4)  # NA means not available

# create a character vector
b <-  c("PS115", "PS509", "PS300", "PS938", "PS9457")

# create a logical vector
c <- c(TRUE, FALSE, TRUE, FALSE, FALSE)  # must all be caps!
```


we can combine them into a data.frame using:

```{r}
# create a data frame with the vectors a, b,and c that we just created
my_dataframe <- data.frame(a,b,c)

# we could also change the column names (currently they are a, b, c)
colnames(my_dataframe) <- c("some_numbers", "my_modules", "logical_values")

# now let's have a look at it
my_dataframe
```

Although note that in most cases we would probably import a dataframe from an external data file, for example using the functions `read.table` or `read.csv`.

---

### Basic plotting in R

We can create plots using the function `plot()`. For example:

```{r, fig.width=4, fig.height=4.2, fig.align='center'}
x = 1:10
y = 3*x - 5
plot(x, y)
```





### Other operations

#### Random number generation

Generate uniformly distributed random numbers (function `runif()`)

```{r, fig.width=4, fig.height=4.2, fig.align='center'}
x <- runif(100, min = 0, max = 1)
hist(x)
```


Generate numbers from a normal distribution

```{r, fig.width=4, fig.height=4.2, fig.align='center'}
y <- rnorm(100, mean = 0, sd = 1)
hist(y)
```


### Getting help

R has a lot of functions, and extra packages that can provides even more. It may seem a bit overwhelming, but it is very easy to get help about how to use a function: just type in a question mark, followed by the name of the function. For example, to see the help of the function we used above to generate the histogram, type 

```{r}
?hist
```


---

$$\\[1in]$$

## Resources for learning R

There is plenty of resources on the web to learn R. I will recommend a couple that I think are particularly well-done and useful:

- [Software Carpentry tutorials on R for Reproducible Scientific Analysis](https://swcarpentry.github.io/r-novice-gapminder/)
- The free book [Learning Statistics with R](https://learningstatisticswithr.com/) by [Danielle Navarro](https://djnavarro.net/)


<!--chapter:end:02-intro-R.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Linear models {#linear-models}

This section will provide some worked examples of how to do analyses in R.

---

## Simple linear regression

In this example^[Taken from Simon Wood's book on GAM[@wood_gam].] we will see how to import data into R and perform a simple linear regression analysis. 

According to the standard big-bang model, the universe expands uniformly and locally, according to Hubble's law[@hubble]
$$
\text{velocity} = \beta \times \text{distance}
$$
where $\text{velocity}$ and $\text{distance}$ are the relative velocity and distance of a galaxy, respectively; and $\beta$ is "Hubble's constant"^[Note the Hubble "constant" is a constant only in space, not in time]. Note that this is a simple linear equation, in which $\beta$ indicate how much the variable $\text{velocity}$ changes for each unitary increase in the variable $\text{distance}$.

According to this model $\beta^{-1}$ gives the approximate age of the universe, but $\beta$ is unknown and must somehow be estimated from observations of $\text{velocity}$ and $\text{distance}$, made for a variety of galaxies at different distances from us. Luckily we have available data from the Hubble Space Telescope. Velocities are assessed by measuring the Doppler effect red shift in the spectrum of light that we receive from the Galaxies. Distance is estimated more indirectly, by using the discovery that in certain class of stars (Cepheids), which display fluctuations in diameter and temperature over a stable period, there is a systematic relationship between the period and their luminosity.

We can load a dataset of measurements from the Hubble Space Telescope in R using the following code
```{r}
d <- read.table(file="https://raw.githubusercontent.com/mattelisi/RHUL-stats/main/data/hubble.txt", 
                header=T)
```

`read.table` is a generic function to import dataset in text files (e.g. .csv files) into R. We use the argument `header=T` to specify that the first line of the dataset gives the names of the columns. Note that the argument `file` here is a URL, but it could be also a path to a file in our local folder. To see the help of this function, and what other arguments and features are available type `?read.table` in the R command line.

We can use the command `str()` to examine what we have imported
```{r}
str(d)
```
This tells us that our data frame has 3 variables:

- `Galaxy`, the 'names' of the galaxies in the dataset
- `velocity`, their relative velocity in Km/sec
- `distance`, their distance expressed in Mega-parsecs^[$1 \text{Mega-parsec} = 3.09 \times 10^{19} \text{Km}$]

We can plot^[See `?plot` for more info about how to customize plots in R.] them using the following code:
```{r, fig.width=4, fig.height=4.2, fig.align='center'}
plot(d$distance, # indicate which variable on X axis
     d$velocity, # indicate which variable on Y axis
     xlab="Distance [Mega-parsecs]",
     ylab="Velocity [Km/sec]", 
     pch=19) # set the type of point
```

It is clear, from the figure, that the observed data do not follow Hubble's law exactly, but given the how these measurements were obtained (there is uncertainty about the true values of the distance and velocities) it would be surprising if they did. Given the apparent variability, what can be inferred from these data? 
In particular:

1. what value of $\beta$ is most consistent with the data? 
2. what range of $\beta$ values is consistent with the data? 

In order to make inferences we make some assumptions about the nature of the measurement noise. Specifically, we assume that measurements errors are well-characterized by a Gaussian distribution. This result in the following model: 
\begin{align*}
y &= \beta x + \epsilon \\
\epsilon &\sim \mathcal{N} \left(0, \sigma_{\epsilon}^2 \right)
\end{align*}
which is essentially a linear regression but without the intercept: that is, whereas normally a linear regression model include an additive term that is not multiplied with the predictor (as in $y = \beta_0 + \beta_1 x + \epsilon$), which gives the expected value of the dependent variable when all predictors are set to zero, in this case the theory tells us we can assume the intercept (the term $\beta_0$) is zero and we can ignore it.


We can fit the model with the function `lm` in R. Note that to tell R that I don't want to fit the intercept, I include in the formula the term `0 + ` - this essentially tells R that the intercept term is set to zero^[A similar results would have been obtained using the notation `velocity ~ -1 + distance`.]

```{r, fig.width=4, fig.height=4}
hub.m <- lm(velocity ~ 0 + distance, d)
summary(hub.m)
```

So, based on this data, **our estimate of the Hubble constant is `r round(hub.m$coefficients, digits=2)` with a standard error of `r round(summary(hub.m)$coefficients[2],digits=2)`.** The standard error - which is the standard deviation of the sampling distribution of our estimates - gives an ideas of the range of values that is compatible with our data and could be used to compute a confidence intervals (roughly, we would expect that the 'true' values of the parameters lies in the interval defined by $\pm$ 2 standard errors 95% of the times).


::: {.rmdnote}

*So, how old is the universe?*

The Hubble constant estimate have units of $\frac{\text{Km}/\text{sec}}{\text{Mega-parsecs}}$. A Mega-parsecs is $3.09 \times 10^{19} \text{Km}$, so we divide our estimate of $\hat \beta$ by this amount. The reciprocal of $\hat \beta$ then gives the approximate age of the universe (in seconds). In R we can calculate it (in years) as follow

```{r}
# transform in Km
hubble.const <- coef(hub.m)/(3.09 * 10^(19))

# invert to get age in seconds
age <- 1/hubble.const

# use unname() to avoid carrying over 
# the label "distance" from the model
age <- unname(age)

# transform age in years
age <- age/(60^2 * 24 * 365)

# age in billion years
age/10^9
```

giving an estimate of about 13 billion years.

:::




<!--chapter:end:03-linear-models.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Models for count data {#count-data}

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
```


This section will provide some examples of models that can deal with *count* data. Typically, count data occurs when the dependent variable is the counted number of occurrences of an event - for example the number of patients arriving in an emergency department (A&E) in a given time of the day - e.g. between 10:00 and 11:00. In this case, the dependent variable (the number of patients) has several characteristics that make it unsuitable for analysis with standard linear models such as linear regression: their distribution is discrete, composed only of non-negative integers, and is often positively skewed, with many observations having a value of 0. 

Another characteristic is that the variance of the observations (e.g. the variance of the number of counts across observations within the same condition) increases with their expected value (e.g. the average number of counts for that condition)^[This makes sense if you think that when the expected number of counts is very low, say $\approx 1$, there cannot be many observations with very high counts - otherwise their average wouldn't be as low (recall that counts are strictly non-negative). In other words, the variance must be low when the average is also low.].

## Poisson model

The simplest model that account for the characteristics mentioned above is a generalized linear model that assume a dependent variable with a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution). The Poisson distribution has a single free parameter, usually notated with $\lambda$, which gives both the expected value (the mean) and the variance of the count variable. In fact it assumes that the variance has the same value as the mean. 

Formally, a Poisson model is usually formulated as follow: let $y=(y_1, \dots, y_n)$ be the dependent variable, consisting of counts (non-negative integers) and $x$ the independent variable (predictor). Then

$$
y_i \sim \text{Poisson} \left(\lambda_i \right)
$$
where^[This is the most common formulation, and is sometime referred to as log 'link' function, to indicate the fact that we have a function (the logarithm function) that 'link' the linear part of the model (the linear combination of the independent variables, here $\beta_0 + \beta_1 x_i$) with the parameter of the distribution of the dependent variable (here the Poisson rate parameter $\lambda$). This is a common component to all generalized linear models, for example for the logistic regression we have a 'logit' link function - the quantile function of the standard logistic distribution - that link the linear predictor part with the parameter $p$ of the binomial distribution.]
$$
\log( \lambda_i) = \beta_0 + \beta_1 x_i
$$
or alternatively 
$$
\lambda_i = e^{\beta_0 + \beta_1 x_i}
$$
Which indicates that we can use the exponential function (in R the function `exp()`) to calculate the predicted values of the mean (more precisely, the expected value, $\mathbb{E}(y)$) and variance ($Var(y)$) of the dependent variable. This relies on the property of the Poisson distribution^[The symbol $\implies$ is used to denote logical implication, e.g. $(A=B)\implies (B=A)$ - symmetry of logical equivalence.]
$$
y_i \sim \text{Poisson} \left(\lambda \right) \implies \mathbb{E}(y)=Var(y)=\lambda
$$


However, in practice, data often do not conform to the constraint of having identical mean and variance. Often the observed variance of the count is higher than what predicted according to the Poisson model (we say in this case that the data is _over-dispersed_).

::: {.rmdnote}

*How does over-dispersion look like?*

The type of data expected under a Poisson model is illustrated in the figure below, which shows 100 datapoints simulated from the model $y_i \sim \text{Poisson} \left(e^{1 + 2x_i }\right)$. The vertical deviations of the datapoints from the line are consistent with the property of the Poisson distribution that the variance of hte count has the same value as their expected value, formally, $Var(y) = \mathbb{E}(y)$).

```{r, fig.width=4, fig.height=4.2, fig.align='center'}
set.seed(2)
n <- 100
x <- runif(n, -1.3, 1.3)
a <- 1
b <- 2
linpred <- a + b*x # linear predictor part
y <- rpois(n, exp(linpred)) # simulate Poisson observations

plot(x,y,col="blue") # plot
x_ <- seq(-2,2,0.1)
lines(x_, exp(a+b*x_))
segments(x,exp(a+b*x),x,y, lwd=1,lty=3, col="red")
```

We can adjust the code above to simulate the same data with some degree of over-dispersion, using a negative binomial distribution, for different values of the precision parameter theta ($\theta$), which regulate the degree of overdispersion. Importantly, these datapoints are simulated assuming the same function fo the average (expected) number of counts (same also as the previous figure), they just differe in the amount of overdispersion relative to a Poissone model. Note that for value arbitrarily large of the precision parameter $\theta \rightarrow \infty$ (bottom-right panel) the negative binomial converges to the Poisson.


```{r, fig.width=6, fig.height=6, fig.align='center', echo=FALSE}
set.seed(4)
n <- 100
x <- runif(n, -2, 2)
a <- 1
b <- 2
linpred <- a + b*x # linear predictor part

y0 <- rnbinom(n, mu=exp(linpred), size=0.5) # simulate negative binomial
y1 <- rnbinom(n, mu=exp(linpred), size=1) # simulate negative binomial
y25 <- rnbinom(n, mu=exp(linpred), size=10) # simulate negative binomial
yInf <- rnbinom(n, mu=exp(linpred), size=Inf) # simulate negative binomial

par(mfrow=c(2,2))
x_ <- seq(-2,2,0.1)

plot(x,y0,col="blue", ylim=c(0,400), main=expression(theta==0.5),ylab="y") # plot
lines(x_, exp(a+b*x_))
segments(x,exp(a+b*x),x,y0, lwd=1,lty=3, col="red")


plot(x,y1,col="blue", ylim=c(0,400), main=expression(theta==1),ylab="y") # plot
lines(x_, exp(a+b*x_))
segments(x,exp(a+b*x),x,y1, lwd=1,lty=3, col="red")


plot(x,y25,col="blue", ylim=c(0,400), main=expression(theta==10),ylab="y") # plot
lines(x_, exp(a+b*x_))
segments(x,exp(a+b*x),x,y25, lwd=1,lty=3, col="red")

plot(x,yInf,col="blue", ylim=c(0,400), main=expression(theta %->% infinity),ylab="y") # plot
lines(x_, exp(a+b*x_))
segments(x,exp(a+b*x),x,yInf, lwd=1,lty=3, col="red")
```

:::


## Negative binomial model

As mentioned above, data are often overdispersed relative to the Poisson (that is, their variance is larger than the mean). This is an issue because when the data are overdispersed then our results may be largely influences by few extreme datapoints. Moreover, we will have the wrong estimated about the variability of the data. To account for overdispersion we can use the negative binomial which can be seen as a generalization of the Poisson model^[The name _negative binomial_ comes from a sampling procedure that give rise to this distribution: basically the negative binomial gives the probability of the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures occur (this pre-fixed number of failures is a parameter of the negative binomial under an alternative parametrization of this distribution).].

Formally, the negative binomial model with 1 predictor $x$ can be notated as
$$
\begin{aligned}
y_i & \sim \text{NegBinomial} \left(\lambda_i, \theta \right)\\
\log( \lambda_i) & = \beta_0 + \beta_1 x_i
\end{aligned}
$$

The negative binomial is very similar to the Poisson - in particular it still the case that $\mathbb{E}(y)=\lambda$. However, it includes an additional precision (or "reciprocal dispersion") parameter which I referred to as $\theta$^[I choose 'theta' for consistency with the R output but note that this is sometime referred to as $\phi$ in the literature `¯\_(ツ)_/¯` ]. Essentially, whereas for the Poisson we had that $Var(y) = \mathbb{E}(y)$, now we have that
$$
Var(y) = \mathbb{E}(y) + \frac{\mathbb{E}(y)^2}{\theta}
$$


## Examples 

### Anchoring and alcohol units

To see how this would work in practice, I use negative binomial to analyse a dataset that is based on an experiment run by Ryan McKay and his MSc students^[Note that this is not the actual dataset that they collected but just some data that I simulated based on their research idea.]. 

The goal of the study was to use the anchoring effect to:

> _...to see if we could attenuate under-reporting of alcohol consumption (which is a medical problem). Participants in a high-anchor condition were asked “do you drink more or less than 40 units of alcohol a week”, and were then asked to estimate exactly how many units they’d consumed. Those in a low anchor condition were initially asked “do you drink more or less than 4 units of alcohol a week” before giving their precise estimate, and those in a control condition just gave their precise estimate._

In R we begin by loading the data

```{r}
d <- read_csv("../data/nb_units.csv", show_col_types = F) # data availabel in the data folder of the repository
d
```

We can calculate the mean and variance of the number of units reported in each conditions. This reveal that the variance across participants in the number of units reported is many times higher than the mean in the number of reported units.

```{r}
d %>%
  group_by(condition) %>%
  summarise(Mean = mean(units),
            Variance = var(units)) %>%
  knitr::kable(digits=2,
               caption="Mean and Variance of weekly units of alcohol reported.")
```

We can use ggplot2 library to visualize the distributions of reported units in each condition. We can see tha the distribution are skewed and contains many 0, which would make them unsuitable for an analysis with a linear regression model.


```{r, fig.align='center', fig.width=5, fig.height=4}
d %>%
  ggplot(aes(x=units, fill=condition)) +
  geom_histogram(binwidth=1)+
  facet_grid(condition~.) +
  theme_minimal() 
```


To estimate the negative-binomial model, we can use the function `glm.nb()` from available in the `MASS` package. Our predictor condition is categorical with 3 levels and therefore it is coded internally as a set of 2 dummy variables. We can see how the contrast is set using

```{r}
d$condition <- factor(d$condition) # tell R that this is a categorical factor
contrasts(d$condition)
```

This indicate that `control` is our baseline condition and the model will have 2 coefficients that code for the difference in the `high` and `low` anchoring condition relative to the control one.

Note also that for this analysis the variable `units` must contain only integer values - if participants reported non-integer values (e.g. a bottle of lager is about 1.7 units) we could divide everything by the minimum common denominator so that we end up with integer values. 

The following command can be used to estimate the model and examine the results

```{r}
library(MASS)
nb01 <- glm.nb(units ~ condition, data = d)
summary(nb01)
```

We can see from output that the condition `high` anchoring elicited reports with higher number of alcohol units than the `control` condition.

We can use the model to make a more precise statement about the size of the difference. We can use the value of the coefficients to calculate the predicted values of counts. The exact values of the coefficients can be accessed from the fitted model using the `$` operator

```{r}
nb01$coefficients
```

The values are combined together according to the dummy variables coding for the condition and represents the linear predictor part of the model:

$$
\lambda_i = \exp \left(\beta_0 + \beta_1\times D_\text{high}+ \beta_2\times D_\text{low} \right)
$$
where I have used the notation $D_\text{high}$ and $D_\text{low}$ to indicate the two dummy variable, whose value is 1 for observation in the `high` and `low` conditions, respectively, and zero otherwise. 

$\beta_0$ is a common notation for the intercept parameter - in this case it gives the expected number of alcohol units in the control condition (because for observations in the control condition we have that $D_\text{high}= D_\text{low}=0$). Thus our model predict an average number of counts in the control condition of 

```{r}
exp(nb01$coefficients["(Intercept)"]) # equivalent to exp(nb01$coefficients[1])
```

(Compare this value with the table above).


Furthermore, our models tells us also that the number of reported alcohol units increase multiplicatively in the `high` condition by a factor of 

```{r}
exp(nb01$coefficients["conditionhigh"])
```

In fact the predicted number of counts in the `high` condition can be derived from the model as 

```{r}
exp(nb01$coefficients["(Intercept)"])  * exp(nb01$coefficients["conditionhigh"]) 
```
 
or equivalently 
 
```{r}
exp(nb01$coefficients["(Intercept)"] + nb01$coefficients["conditionhigh"]) 
```
 
Finally, note that we can use the `sjPlot` library to prepare a fancy version of the model output, and we can see that the multiplicative factor that describe the increase in reported units is called here an _incidence ratio_^[Although honestly I am not sure how common is this terminology].

```{r}
library(sjPlot)
tab_model(nb01)
```


#### Adding predictors

The dataset include also information about the gender of the participants. We may hypothesize that male participants drink more than female ones^[For the sake of the example we use only 2 gender categories, but in a real study we should be mindful to include more options for non-binary / third gender participants, as McKay's students did in the real study.]. Does taking this into account improve the accuracy of our modelling?
  
  To test this, we can estimate an additional model with also gender as predictor. We can compare this to the previous one using a *likelihood-ratio test*. This is based on a [theorem](https://en.wikipedia.org/wiki/Wilks%27_theorem) which states that the difference in log-likelihood^[In this model the parameters are estimated via maximum likelihood, which amounts to choosing the values of the parameters that maximize the probability (likelihood) of the data under the model. Thus when we refer to the log-likelihood of a model we indicate the logarithm of the maximized value of the likelihood function.] between nested models is (asymptotically) distributed according to a Chi-squared distribution, therefore allowing the calculation of a p-value. In R this can be done using the function `anova()`.

First, let's fit an additional model with the extra predictor `gender`

```{r}
nb02 <- glm.nb(units ~ condition + gender, data = d)
summary(nb02)
```

This indicate that indeed male participants report on average 

```{r}
exp(nb02$coefficients["genderMale"])
```

times more units of alcohol per week than females.

We can already see that the difference due to gender is significant, but nevertheless let's compare them using a likelihood ratio test.

```{r}
anova(nb01, nb02)
```

Here the value of the likelihood ratio statistic is `r round(anova(nb01, nb02)$"LR stat.",digits=2)` and under the null hypothesis that any improvement of model fit obtained after adding gender as predictor is due to chance is distributed as a Chi-square with 1 degree of freedom.


#### Plotting model fit

It's not straightforward to visualize the model fit to the data - the code below give one possibility:

```{r, fig.align='center', fig.width=5, fig.height=4, fig.cap="The black line represent the predicted probability of the data (note that I clipped the x-axis at 40)"}
# here I make a new data matric for claculating the prediction of the model
nd <- expand.grid(condition=unique(d$condition), 
                  units = 0:max(d$units),
                  KEEP.OUT.ATTRS = F)

# use the predict() function to calculate the predicted counts for each condition
nd$predicted_units <- predict(nb01, newdata=nd, type="response")

# here I use the dnbinom() function - which gives the probability density of 
# the negative binomial - to calculate the probability of the observations under the model
nd$pred_density <- dnbinom(nd$units, mu=nd$predicted_units, size=nb01$theta)

# finally take all together and plot
d %>%
  ggplot(aes(x=units, fill=condition)) +
  geom_histogram(aes(y=..density..),binwidth=1, color="white")+
  geom_line(data=nd, aes(x=units, y=pred_density),size=1) +
  facet_grid(condition~.) +
  theme_minimal() +
  coord_cartesian(xlim=c(0,40))
```


Admittedly the probability of the data under the model (the black lines) looks quite similar across the three panels, however, the model does assign higher probability to higher count values in the `high` condition compared to the other ones - we can see this by putting them together in the same panel, and by plotting the logarithm of the probability instead of the probability itself. These changes in the probability of the data may not seems large when looked at in this way, but they amount to quite substantial changes in the average number of counts - recall that in the `high` condition participants reported on average nearly 1.6 times the number of alcohol units than in the control condition. 

```{r, fig.align='center', fig.width=5, fig.height=4}
nd %>%
  ggplot(aes(x=units, y=log(pred_density), color=condition))+
  geom_line(size=0.6)+
  theme_minimal() +
  coord_cartesian(xlim=c(0,40),ylim=c(-7,-1.4))+
  labs(y="log (probability)")
```










<!--chapter:end:04-count-data.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Models for ordinal data {#ordinal}

Ordinal-type of variable arise often in psychology. One common example are responses to Likert scales. Although it is very common practice that these are analyzed with a linear model, it is know that this approach can lead to serious inference errors [@Liddell2018]. For this reason, the recommended approach is to use a model appropriate for ordinal data. Here I will describe an approach to this, using an ordered logistic regression model (also know as proportional odds model). 

## Ordered logistic regression

One way to think about this model is by assuming the existence of a continuous latent quantity, call it $y$, specified by a logistic probability density function. The latent distribution is partitioned into a series of $k$ intervals, where $k$ is the number of ordered choice options available to respondents, using $k+1$ latent cut-points, $c_1, \ldots, c_{k+1}$. By integrating the latent density function within each interval we obtain the ordinal response probabilities $p_1, \ldots, p_k$. Other choices are possible (e.g. assuming a normally distributed latent variable would yield an ordered _probit_ model). Beyond mathematical convenience, one advantage of the ordered logit is that coefficient can be interpreted as ordered log-odds, implementing the proportional odds assumption [@McCullagh1980].

Formally, the model can be notated as

$$
\begin{aligned}
p_k & = p\left(c_{k-1} < y \le c_k \mid \mu \right)\\
 & = \text{logit}^{-1}\left(c_k - \mu \right) - \text{logit}^{-1}\left(c_{k-1} - \mu \right)
\end{aligned}
$$

where 
$$
\text{logit}^{-1}(\alpha) = \frac{1}{1+e^{-\alpha}}
$$
is the cumulative function of the logistic distribution (also known as inverse-logit), and
$$
\mu = \beta_1 x_1 + \ldots + \beta_n x_n
$$
is the linear part of the model (a linear combination of the $n$ predictor variables).


This is the general approach and the formalism used - below I present few examples that illustrates how this work in practice in R.


### Mixed-effects ordinal regression

R libraries used in this example

```{r, message=FALSE}
library(ggplot2)
library(ordinal)
library(tidyverse)
library(DescTools)
```

In addition to the above libraries, here I will create a handy R function that gives the probabilities of the categorical responses given a mean value of the latent quantity (indicated with $\mu$ above) and a set of cutpoints $c_1, \ldots, c_{k+1}$. This will be used both for simulating the data and for plotting the fit of the model.

```{r}
ordered_logistic <- function(eta, cutpoints){
  cutpoints <- c(cutpoints, Inf)
  k <- length(cutpoints)
  p <- rep(NA, k)
  p[1] <- plogis(cutpoints[1], location=eta, scale=1, lower.tail=TRUE)
  for(i in 2:k){
    p[i] <- plogis(cutpoints[i], location=eta, scale=1, lower.tail=TRUE) - 
      plogis(cutpoints[i-1], location=eta, scale=1, lower.tail=TRUE)
  }
  return(p)
}
```

For this example we simulate some data. We have two predictors: `x1`, a continuous predictor that vary with each observation, and `d1` a dummy variable that indicate a categorical predictor with 2 levels (e.g. two experimental conditions). The conditions are within-subject, meaningthat each participant (identified by the variable `id`) is being tested in both conditions.


```{r}
set.seed(5)
N <- 200
N_id <- 10
dat <- data.frame(
  id = factor(sample(1:N_id,N, replace = T)),
  d1 = rbinom(N,1,0.5), # dummy variable (0,1) indicate 2 conditions
  x1 = runif(n = N, min = 1, max = 10)
)
rfx <- rnorm(length(unique(dat$id)), mean=0, sd=5)
LP <- 0.5*dat$x1 + 2*dat$d1 + rfx[dat$id]
for(i in 1:N){
  dat$response[i] <- which(rmultinom(1,1, ordered_logistic(LP[i], c(0,2.5, 5,10)))==1)
}
dat$response <- factor(dat$response)
str(dat)
```

The dependent variable is categorical with 5 levels - here is a plot of the number of responses per category in the two conditions. We are interested in testing whether the distribution differ across the conditions.

```{r,fig.height=4,fig.width=6,fig.align='center'}
ggplot(dat,aes(x=response))+
  geom_bar()+
  facet_grid(.~d1)
```

We use the `clmm()` function in the package `ordinal` to estimate the model. The syntax is similar to what we would use for a linear mixed effect model. Note that in the output the `Threshold coefficients` are the latent cutpoints $c_1, \ldots, c_{4}$

```{r}
model <- clmm(response ~ x1 + d1 + (1|id), data = dat)
summary(model)
```

There is no function that can out-of the box calculate the predictions of the model for us, so this will need some coding. I also use the library `DescTools` to calculate simultaneous multinomial confidence intervals. In the resulting plot the black line are model fit, and bar the observed responses.

```{r,fig.height=4,fig.width=6,fig.align='center'}
# pre-allocate a matrix to store model predictions
# note that these are a vector of 5 probabilities for each trial
pred_mat <- matrix(NA, nrow=N, ncol=length(unique( dat$response)))

for(i in 1:N){
  
  # first calculate the linear predictor 
  # by summing all variable as indicated
  # in the model formulate, weighted by the coefficients
  eta <- dat$x1[i]*model$beta['x1'] +  dat$d1[i]*model$beta['d1'] + model$ranef[dat$id[i]]
  # note that + model$ranef[dat$id[i]] adds 
  # the random intercept for the subjects of observation i
  
  # calculate vector of predicted probabilities
  pred_mat[i,] <- ordered_logistic(eta, model$Theta)
  
}

# add predictions to dataset
pred_dat <- data.frame(pred_mat)
colnames(pred_dat) <- paste("resp_",1:ncol(pred_mat),sep="")
pred_dat <- cbind(dat, pred_dat)

# in order to visalize the predictions, 
# we first average them for each condition
pred_dat %>%
  pivot_longer(cols=starts_with("resp_"), 
               names_prefix="resp_",
               values_to = "prob",
               names_to ="response_category") %>%
  group_by(d1, response_category) %>%
  summarise(prob = mean(prob),
            n=sum(response==response_category)) %>%
  group_by(d1) %>%
  mutate(prop_obs = n/sum(n),
    response=as.numeric(response_category)) -> pred_d1

# cimpute the multinomial interval
pred_d1$CI_lb <- MultinomCI(pred_d1$n)[,"lwr.ci"] *2 
pred_d1$CI_ub <- MultinomCI(pred_d1$n)[,"upr.ci"] *2
# note that I multiply for 2 because in the plot each condition 
# will be in a different panel and the probability will sum to 1 in each panel

# visualize (aggregated) ordinal response & prediction
# the black line are the predictions of the model

ggplot(pred_d1,aes(x=response, y=prop_obs))+
  geom_col()+
  geom_errorbar(data=pred_d1, aes(ymin=CI_lb, ymax=CI_ub),width=0.2)+
  facet_grid(.~d1)+
  geom_line(data=pred_d1, aes(y=prob), size=2)+
  labs(y="probability")
```

We can repeat the same process also for calculating predictions for individual participants

```{r,fig.height=8,fig.width=4,fig.align='center'}
# split by ID
pred_dat %>%
  pivot_longer(cols=starts_with("resp_"), 
               names_prefix="resp_",
               values_to = "prob",
               names_to ="response_category") %>%
  group_by(id, d1, response_category) %>%
  summarise(prob = mean(prob),
            n=sum(response==response_category)) %>%
  group_by(d1, id) %>%
  mutate(prop_obs = n/sum(n),
         response=as.numeric(response_category))  -> pred_d1

# calculate multinomial CI
# we do a loop over all participants and conditions
pred_d1$CI_lb <- NA
pred_d1$CI_ub <- NA
for(i in unique(pred_d1$id)){
  for(cond in c(0,1)){
    pred_d1$CI_lb[pred_d1$id==i & pred_d1$d1==cond] <- DescTools::MultinomCI(pred_d1$n[pred_d1$id==i & pred_d1$d1==cond])[,"lwr.ci"] 
    pred_d1$CI_ub[pred_d1$id==i & pred_d1$d1==cond] <- DescTools::MultinomCI(pred_d1$n[pred_d1$id==i & pred_d1$d1==cond])[,"upr.ci"]
  }
}

pred_d1 %>%
  mutate(condition = factor(d1)) %>%
  ggplot(aes(x=response, y=prop_obs, fill=condition))+
    geom_col()+
    geom_errorbar(aes(ymin=CI_lb, ymax=CI_ub, color=condition), width=0.2)+
    facet_grid(id~d1)+
    geom_line(aes(y=prob), size=2)+
    labs(y="probability")
```






<!--chapter:end:04-ordinal.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Meta-analyses {#meta-analysis}

For running meta-analyses, we recommend the `metafor` package for R (see [link 1](https://www.metafor-project.org/doku.php), [link 2](https://wviechtb.github.io/metafor/)).

A comprehensive, hands-on guide on how to use this package is provided in the book by Harrer and colleagues [@harrer2021doing], freely available at this [link](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/).

An alternative to the `metafor` package is to Bayesian multilevel modelling (also discussed in the book by Harrer and colleagues). A more technical discussion of Bayesian multilevel modelling for meta-analyes is provided in [this paper](https://psyarxiv.com/7tbrm/) by Williams, Rast and Bürkner [@williams_rast_bürkner_2018].


<!--chapter:end:08-meta-analysis.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Missing data {#missing-data}


## Types of missing data

Following the work of Rubin[@rubin76], missing data are typically grouped in 3 categories:

- Missing completely at random (**MCAR**). This assumes that the probability of being missing is the same for all cases; this implies that the mechanisms that causes missingness is not related in any way to the data. For example, say, there's a known unpredictable error on the server side that prevented recording some responses for some respondents to a survey. As the missingness is entirely independent on the respondents' characteristics, this would be MCAR. When the data are MCAR we can ignore a lot of the complexities and just do a _complete-case_ analysis (that is, simply exclude incomplete observations from the dataset). A part from possible loss of information, doing a complete case analysis should not introduce bias in the results. In practice, however, it is difficult to establish whether the data are truly MCAR. Ideally, to argue that data are MCAR,  one should have a good idea of the mechanisms that caused missigness (more on this below). Formally, data is MCAR if $$
\Pr(R=0|Y_\mathrm{obs},Y_\mathrm{mis},\psi) = \Pr(R=0|\psi)
$$ where $R$ is an indicator variable that is set to 0 for missing data and 1 otherwise; $Y_\mathrm{obs},Y_\mathrm{mis}$ indicate observed and missing data, respectively; and $\psi$ is simply a parameter that determine the overall (fixed) probability of being missing.

- Missing at random (**MAR**). A less strong assumption about missingness is that it is systematically related to the observed but not the unobserved data. For example, data are MAR if in a study male respondents are less likely to complete a survey on depression severity than female respondents - that is, the probability of reaching the end of the survey is related to their sex (fully observed) but not the severity of their symptoms. Formally, data is MAR if $$
\Pr(R=0|Y_\mathrm{obs},Y_\mathrm{mis},\psi) = \Pr(R=0|Y_\mathrm{obs},\psi)
$$ When data are missing at random (MAR) the results of complete case analyses may be biased and a common approach to deal with this is to use imputation. Stef van Buuren has a [freely available online book on this topic](https://stefvanbuuren.name/fimd/)[@vanbuuren_imputation]. Among other things, it illustrates how to do multiple imputation in R with examples.

- Missing not at random (**MNAR**). This means that the probability of being missing varies for reasons that are unknown to us, and may depends on the missing values themselves. Formally this means that $\Pr(R=0|Y_\mathrm{obs},Y_\mathrm{mis},\psi)$ does not simplify in any way. This case is the most hard to handle: a complete case analyses may or may not be biased, but there is no way of knowing it and we may have to find more information about what caused missingness. 


## Deciding whether the data are MCAR

As MCAR is the only scenario in which it is safe to do a complete case analysis, it would seem useful to have way to test this assumption. Some approaches have been proposed to test whether the data are MCAR, but they are not widely used and it's not clear how useful they are in practice. For example one could run a logistic regression with "missingness" as dependent variable (e.g. an indicator variable set to 1 if data is missing and 0 otherwise), and all other variables as predictors - if the data are MCAR then none of the predictors should predict missingness. A popular alternative, implemented in several software packages is Little's test[@little88]. 

Technically, these approaches can help determine whether the missingness depends on some observed variables (that is, if they are MAR), but strictly speaking cannot exclude missingness due to unobserved variables (MNAR scenario). Nevertheless, if one has good reasons to believe that the data are MCAR, and want to add some statistical test that corroborate this assumption, these could be reasonable tests to do. However, it remains important to also discuss openly possible reasons and mechanisms of missingness, and explain why we deem it a priori plausible that the data are MCAR. In fact, _statistical tests alone cannot tell whether data are missing completely at random_. The terms MCAR, MAR and MNAR refers to the _causal_ mechanisms that is responsible for missing data and, strictly speaking, causal claims cannot be decided uniquely on the basis of a simple statistical test. If the data "pass" the test it would provide some additional support to the assumption that they are MCAR, but in and of itself the test alone does not fully satisfy the assumptions of MCAR. To see why note that MCAR (as formally defined above) assumes also that there should be no relationship between the missingness on a particular variable and the values of that same variable: but since this is a question about what is missing from the data, it cannot be tested with any quantitative analysis of the available data. Finally, it should be added that as these are null-hypothesis significance test, a failure to reject the null hypothesis does not, in and of itself, provide evidence for the null hypothesis (that the data are MCAR). It may be also that we don't have enough power to reliably detect the pattern in the missingness.



## Causal analysis and Bayesian imputation

The best and most principled approach to deal with missingness (at least in my opinion) is to think hard about the causal mechanisms that may determine missingness, and use our assumption about the causal mechanisms to perform a full Bayesian imputation (that is, treating the missing data as parameter and estimating them). 

I plan to create and include here a worked example of how to do this; in the meantime interested readers are referred to Chapter 15 (in particular section 15.2) of [the excellent book by Richard McElreath _Statistical Rethinking_](https://xcelab.net/rm/statistical-rethinking/)[@statrethinking] which present a very accessible worked example of how to do this in R.





<!--chapter:end:09-missing-data.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Signal Detection Theory {#SDT}


Signal Detection Theory (hereafter abbreviated as SDT) is probably the most important and influential framework for modelling perceptual decisions in forced-choice tasks, and has wide applicability also beyond purely perceptual decision tasks. Here I review and derive some fundamental concepts of equal-variance and unequal variance signal detection theory, and present some R code to simulate the confidence of an optimal observer/decision-maker.

## Equal-variance Gaussian SDT

SDT relies on the idea that information available to the observer / decision-maker can be modeled as a single random variable on a latent decision space. SDT is typically (but not only) applied to detection tasks: each trials a stimulus is presented, consisting of some background noise with or without a signal. The observer is tasked with deciding whether the signal was present or absent (thus there are 2 possible responses, _yes_ and _no_). SDT assumes that in each trial the observer makes their decision on the basis of a random variable, call it $X$, which may be drawn from the signal distribution or from the noise distribution.

In the simplest case, the signal distribution is a Gaussian with variance $\sigma^2=1$ and mean $d'>0$.

$$
\begin{aligned}
f_S(x)&=\frac{1}{\sigma\sqrt{2 \pi}} e^{-\frac{(x-d')^2}{2 \sigma^2}}\\
&=\frac{1}{\sqrt{2 \pi}} e^{-\frac{(x-d')^2}{2}}
\end{aligned}
$$

And the noise distribution is a second normal distribution with mean $0$ and variance $\sigma^2=1$

$$
f_N(x)=\frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}
$$

Note that the _prior probability_ of signal and noise may not be equal. Let's define the probability of a signal-present trial as $p(S)=\alpha$; we have thus that $p(N)=1-p(S)=1-\alpha$.

### Optimal decision rule

The optimal way to decide whether a particular value of $x$ was drawn from a signal or noise distribution is by using a likelihood-ratio, that is one should responde "yes" whenever

$$
\begin{aligned}
\frac{f_S(x)\,p(S)}{f_N(x) \, p(N)} & \ge 1 \\
\frac{f_S(x)\,\alpha}{f_N(x) \, (1-\alpha)} & \ge 1 
\end{aligned}
$$

With some algebraic manipulations, it can be shown that the likelihood ratio decision rule amounts to comparing the value of $x$ to a criterion $c$:

$$
\begin{aligned}
\frac{f_S(x)\alpha}{f_N(x)(1-\alpha)} & \ge 1 \\
\frac{f_S(x)}{f_N(x)} & \ge \frac{1-\alpha}{\alpha} \\
\frac{\frac{1}{\sqrt{2 \pi}} e^{-\frac{(x-d')^2}{2}}}{\frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}} & \ge \frac{1-\alpha}{\alpha} \\
\frac{e^{-\frac{(x-d')^2}{2}}}{e^{-\frac{x^2}{2}}} & \ge \frac{1-\alpha}{\alpha} 
\end{aligned}
$$

taking the log of both sides

$$
\begin{aligned}
 \log \left(\frac{e^{-\frac{(x-d')^2}{2}}}{e^{-\frac{x^2}{2}}} \right) & \ge \log \left(\frac{1-\alpha}{\alpha}\right) \\
  \log \left(e^{-\frac{(x-d')^2}{2}} \right) - \log \left(e^{-\frac{x^2}{2}} \right) & \ge \log \left(\frac{1-\alpha}{\alpha}\right) \\
-\frac{(x-d')^2}{2} + \frac{x^2}{2}  & \ge \log \left(\frac{1-\alpha}{\alpha}\right) \\
\frac{-x^2 -(d')^2 + 2d' x + x^2 }{2} & \ge \log \left(\frac{1-\alpha}{\alpha}\right) \\
\frac{-(d')^2 + 2d'}{2}x & \ge \log \left(\frac{1-\alpha}{\alpha}\right) \\
d'x -\frac{(d')^2}{2}& \ge \log \left(\frac{1-\alpha}{\alpha}\right) \\
d'x & \ge \log \left(\frac{1-\alpha}{\alpha}\right) + \frac{(d')^2}{2}\\
x & \ge \frac{1}{d'}\log \left(\frac{1-\alpha}{\alpha}\right)+ \frac{d'}{2}
\end{aligned}
$$

The optimal criterion is thus found as $c = \frac{1}{d'}\log \left(\frac{1-\alpha}{\alpha}\right)+ \frac{d'}{2}$. Whenever the $x$ exceed the criterion, $x \ge c$, the observer should respond "signal present" and respond "signal absent" otherwise.

It is easy to verify that when the signal and noise trials are equiprobable, that is $p(s) = p(N) \implies \alpha=0.5$, the optimal criterion becomes $c=\frac{d'}{2}$.


---

#### Visualizing signal detection theory in R

We use R to verify visually that the optimal criterion (represented by the vertical red line) is located at horizontal coordinates of the crossover point of the two probability densities:

```{r,fig.height=4,fig.width=6,fig.align='center'}
# settings
d_prime <- 2
sigma <- 1   
alpha <- 0.5

# support of random variable X (for plotting)
supp_x <- seq(-2,4,length.out=500) 

# calculate optima criterion
optimal_c <- 1/d_prime * log((1-alpha)/alpha) + d_prime/2

# calculate probability density and scale by prior probability
fS <- alpha*dnorm(supp_x, mean=d_prime, sd=sigma)
fN <- (1-alpha)*dnorm(supp_x, mean=0, sd=sigma)

# plot 
plot(supp_x, fS, type="l",lwd=2,col="black",xlab="X",ylab="p(X)")
lines(supp_x, fN, lwd=2,col="dark grey")
abline(v=optimal_c,lwd=1.5,lty=1,col="red")
legend("topleft",c(expression("f"["S"]),expression("f"["N"])),col=c("black","dark grey"),lwd=2,title = "source:",bty="n")
```

(Note that the noise distribution, in dark grey, is centered on zero. The signal distribution is centered on $d'$.)


What if we have unequal prior probabilities, e.g. $p(S)=\alpha=0.8$ ?

The optimal criterion is always at the crossover point, which however is in a different location since the two distribution are scaled by their prior proability.

```{r,fig.height=4,fig.width=6,fig.align='center'}
# Set a different prior probability
alpha <- 0.8

# calculate optima criterion
optimal_c <- 1/d_prime * log((1-alpha)/alpha) + d_prime/2

# calculate probability density and scale by prior probability
fS <- alpha*dnorm(supp_x, mean=d_prime, sd=sigma)
fN <- (1-alpha)*dnorm(supp_x, mean=0, sd=sigma)

# plot 
plot(supp_x, fS, type="l",lwd=2,col="black",xlab="X",ylab="p(X)")
lines(supp_x, fN, lwd=2,col="dark grey")
abline(v=optimal_c,lwd=1.5,lty=1,col="red")
legend("topleft",c(expression("f"["S"]),expression("f"["N"])),col=c("black","dark grey"),lwd=2,title = "source:",bty="n")
```


---

### Estimating the parameters from data

The parameters can be easily estimated form the proportions of hits, $p_\text{H}$, and false alarms $p_{\text{FA}}$. (Hits are correct 'yes' responses and faalse alarms are incorrect 'yes' responses.)

Consider first that the probability of a false alarm is just the probability of observing $X \ge c$ when $X$ is drawn from the noise distribution $f_N$.

Thus

$$
p_{\text{FA}} =  1 - \Phi(c)
$$
where $\Phi$ is the cumulative distribution function of the standard normal distribution It words, $\Phi(c)$ is the area that lies to the left of $c$ and under a Gaussian function with mean $0$ and variance $1$. 

Similarly when $X$ is drawn from the signal distribution, the probability of a hit response is the probability that a $x$ drawn from the signal distribution $f_S$ is greater than the criterion $c$,
$$
p_{\text{H}} =  1 - \Phi(c - d')
$$
Thus, if we know the proportion of hits and false alarms, we can estimate $c$ and $d'$ using the inverse of $\Phi$, which can be notated as $\Phi^{-1}$ and its often referred to as the _quantile function_

$$
c = \Phi^{-1}\left(1 -  p_{\text{FA}}\right) = - \Phi^{-1}\left(p_{\text{FA}}\right) \\
d' = c - \Phi^{-1}\left(1 -  p_{\text{H}}\right) =\Phi^{-1}\left(p_{\text{H}}\right) - \Phi^{-1}\left(p_{\text{FA}}\right) 
$$


---

### GLM formulation of equal-variance SDT models

Note that the above SDT model could be reformulated a _probit_ generalized linear model. This could be expressed as 

$$
\Phi^{-1}(p_{\text{yes}}) = \beta_0 + \beta_1 x
$$
where $p_{\text{yes}}$ is the probability of the observer responding that the signal was present, and $x$ is a variable that indicates the presence/absence of the signal as 1/0, respectively. The similarity with the SDT model is evident if we consider that, in the GLM, the probability of a hit or a false alarm correspond to $p_{\text{yes}}$ when the signal is present (that is $x=1$) or absent (that is $x=0$), respectively. This allows mapping the signal detection theory parameters, $d'$ and $c$ to the GLM intercept and slope parameters, $\beta_0$ and $\beta_1$

$$
c = -\Phi^{-1}(p_{\text{FA}}) = - \beta_0
$$
and 

$$
d' = \Phi^{-1}(p_{\text{H}}) - \Phi^{-1}(p_{\text{FA}}) = \beta_0 + \beta_1 -\beta_0 = \beta_1
$$
Recognizing this identity makes it easier to use statistical packages such as R to easily analyse complex design with multiple conditions and interaction effects. It also makes it possible to estimate multi-level (or hierarchical, random-effects) SDT models.


## Bayesian confidence in equal-variance SDT

Formally, confidence should be the Bayesian (that is, subjective) posterior probability that a decision was correct given the evidence available to the observer.

First some notation. Let's use $S$ to indicate the event that a signal was present (an event is something to which we can assign a probability) and $N$ to indicate the event that only noise was presented. Say the observer respond yes, their confidence should correspond to the posterior probability $p(S\mid x)$, that is the probability that a signal was present given that we observed $x$.

This can be calculated applying Bayes theorem:


$$
\begin{aligned}
p(S \mid x) & = \frac{p(x \mid S)\times p(S)}{p(x)} \\
& = \frac{p(x \mid S) p(S)}{p(x \mid S) p(S)+p(x \mid N) p(N)} \\
& = \frac{p(x \mid S) \alpha}{p(x \mid S) \alpha+p(x \mid N) (1-\alpha)}
\end{aligned}
$$

This can be simplified further in cases where we have equal probabilities $p(s) = p(N) \implies \alpha=0.5$
$$
p(S \mid x)_{\alpha=0.5} =  \frac{p(x \mid S)}{p(x \mid S)+p(x \mid N)}
$$

Note also that the confidence for the _signal absent_ trials $N$ is calculated in the same way:

$$
p(N \mid x)_{\alpha=0.5} =  \frac{p(x \mid S)}{p(x \mid N)+p(x \mid N)}
$$

One question we may ask at this point is how the distribution of confidence levels of the observers changes in correct vs wrong responses, and also in signal absent vs signal present responses. The simplest way to get at this is by simulation - see the following R code

```{r,fig.height=2.5,fig.width=7,fig.align='center'}
# load ggplot library for plotting
library(ggplot2)

# settings
d_prime <- 1.5
sigma <- 1   
alpha <- 0.5

# calculate optimal criterion
optimal_c <- 1/d_prime * log((1-alpha)/alpha) + d_prime/2

# simulate 2*10^4 trials and calculate the confidence 
N_trials <- 2*10^3
tar_pres <- c(rep(0,N_trials/2),rep(1,N_trials/2))

# simulate X by adding Gaussian noise (function rnorm())
x <- tar_pres*d_prime + rnorm(length(tar_pres), mean=0, sd=1)
resp_yes <- ifelse(x >= optimal_c,1,0)

# define a custom function to calculate confidence
confidenceSDT1 <- function(x,resp,d_prime, alpha=0.5){
  conf <- ifelse(
    resp==1,
    dnorm(x,mean=d_prime,sd=1)/(dnorm(x,mean=d_prime,sd=1)+dnorm(x,mean=0,sd=1)),
    dnorm(x,mean=0,sd=1)/(dnorm(x,mean=d_prime,sd=1)+dnorm(x,mean=0,sd=1))
  )
  return(conf)
}

# calculate confidence
confidence <- confidenceSDT1(x, resp_yes, d_prime=1.5)

# put into a dataframe for plotting
d <- data.frame(confidence, x, tar_pres, resp_yes)

# check which simulated responses are correct
d$correct <- ifelse((d$tar_pres==1 & d$resp_yes==1)|(d$tar_pres==0 & d$resp_yes==0),1,0)

# plot
d$tar <- ifelse(d$tar_pres==1,"signal present","signal absent")
d$correct <- ifelse(d$correct==1," correct response","wrong response")
ggplot(d,aes(x=confidence,group=correct,color=correct,fill=correct))+
  geom_histogram(position = 'dodge',aes(y=..density..), binwidth=0.05,alpha=0.9)+
  facet_grid(.~tar)+
  scale_color_manual(values=c("dark green","red"),name="")+
  scale_fill_manual(values=c("dark green","red"),name="")+
  labs(x="confidence level")+
  theme_classic()
```

The distribution of confidence is - as expected - different from correct and wrong response: it is peaked near 1 for correct responses, and peaked near 0.5 for errors. Importantly, the separation between confidence distributions in correct and wrong responses is similar in both signal absent (left panel) and signal present (right panel) trials. This suggest that metacognitive sensitivity - the ability to discriminate between correct and incorrect responses - should not change across signal present and signal absent answers. This can be visualized with Type-2 ROC (Receiver Operating Characteristic) curves, which are obtained by plotting the proportion of "type-2 hits" as a function of the "type-2 false alarms" - these are the fraction of correct and wrong responses that are classified as correct for each possible
threshold setting on the confidence distribution. The term "Type-2" is used to indicate that is a metacognitive task - a decision about a decision[@Galvin2003]. 

```{r,fig.height=5,fig.width=5,fig.align='center'}
# functions to compute true and false positive rates
TPR <- function(d,th){ sum(d$tar_pres==1 & d$x>th) / sum(d$tar_pres==1)}
FPR <- function(d,th){ sum(d$tar_pres==0 & d$x>th) / sum(d$tar_pres==0)}

# use all the sorted values are possible threshods
thresholds <- sort(d$x)

roc <- data.frame(y=sapply(thresholds, function(th){TPR(d[d$resp_yes==1,],th)}), 
                 x=sapply(thresholds, function(th){FPR(d[d$resp_yes==1,],th)}) )

roc0 <- data.frame(y=sapply(thresholds, function(th){TPR(d[d$resp_yes==0,],th)}), 
                 x=sapply(thresholds, function(th){FPR(d[d$resp_yes==0,],th)}) )

ggplot(roc,aes(x,y))+geom_point(color="blue")+theme_classic()+labs(y="Type-2 hits", x="Type-2 FA") +geom_abline(intercept=0,slope=1,lty=2)+geom_point(data=roc0,color="dark red")+ggtitle("Equal-variance SDT, Type 2 sensitivity")
```

The ROC curve derived from confidence ratings discriminate equally well correct vs wrong responses (in the plot, blue is the curve for target present responses) - thus indicating similar metacognitive or "type-2" sensitivity. However, empiricaly it has been found that metacognitive sensitivite seems to be worse for signal absent responses - e.g. see [@Mazor2020]. As we see below, this finding can be accomodated by relaxing the assumption that signal and noise distribution have the same standard deviation, and assuming instead that the standard deviation of the signal present distribution is larger.




---

## Unequal-variance SDT

Signal detection theory can be extended to account for cases in which signal and noise distribution have a different variance. For many types of random processes, the mean and the variance are related such that signals with higher mean have also higher variance (the firing rate of neurons is an example). Thus the signal distribution is now given by

$$
f_S(x)=\frac{1}{\sigma\sqrt{2 \pi}} e^{-\frac{(x-d')^2}{2 \sigma^2}}
$$

with the variance $\sigma^2 \ne 1$.

One important consequence of the different noise level, is that now the log-likelihood ratio is a quadratic function of the signal x

$$
\log \left( \frac{f_S(x)}{f_N(x)} \right) = -\frac{1}{2 \sigma^2} \left[\left(1 - \sigma^2 \right)x^2 -2d' x + d'^2 + 2\sigma^2 \log \sigma\right]
$$
As a result, the log-likelihood ratio crosses zero in 2 points, thus yielding 2 decision criteria - see next figure.

```{r,fig.height=3.5,fig.width=7,fig.align='center'}
# settings
d_prime <- 3
sigmaS <- 2  
sigmaN <- 1 # fixed
alpha <- 0.5


# calculate optimal UEV-SDT criterion
UVGSDTcrit <- function(dp, sig, logbeta=0){
  TwoSigSq <- 2 * sig^2
  
  minLam <- optimize(function(X, dp, sig){-((1 - sig^2) * X^2 - 2 * dp * X + dp^2 + TwoSigSq * log(sig))/TwoSigSq}, c(-10, 10), dp = dp, sig = sig)$objective
  
  if(logbeta < minLam){ warning("complex roots")}
  
  cf <- -c(dp^2 + TwoSigSq * log(sig) + logbeta * TwoSigSq,-2 * dp, 1 - sig^2)/TwoSigSq
  
  proot <- polyroot(cf)
  
  return(sort(Re(proot)))
}

#
UE_c <- UVGSDTcrit(d_prime, sigmaS)

# simulate 2*10^3 trials and calculate the confidence 
N_trials <- 2*10^3
tar_pres <- c(rep(0,N_trials/2),rep(1,N_trials/2))

# simulate X by adding Gaussian noise (function rnorm())
# first generate internal responses
x <- rep(NA,length(tar_pres))
x[tar_pres==0] <- rnorm(N_trials/2, mean=0,sd=1)
x[tar_pres==1] <- rnorm(N_trials/2, mean=d_prime,sd=sigmaS)


# plot unequal variance and criterion
Xi <- seq(-6,10,length.out=500)
fS = dnorm(Xi,mean=d_prime,sd=sigmaS)
fN = dnorm(Xi,mean=0,sd=1)
plot(Xi,fN,type="l",col="grey",lwd=3,ylab="density",xlab="X")
lines(Xi,fS,lwd=3)
abline(v=UE_c,col="red",lwd=2)
```

The reason why there are two criteria may be seen more clearly if we plot the logarithm of the probability density, as this makes it evident that there are two regions, to the left and the right of the noise distribution, in which the probability of signal present is larger than that of no-signal (i.e noise only).

```{r,fig.height=5,fig.width=7,fig.align='center'}
# plot unequal variance and criterion
Xi <- seq(-6,10,length.out=500)
fS = dnorm(Xi,mean=d_prime,sd=sigmaS)
fN = dnorm(Xi,mean=0,sd=1)
plot(Xi,fN,type="l",col="grey",lwd=3,ylab="density",xlab="X", log="y")
lines(Xi,fS,lwd=3)
abline(v=UE_c,col="red",lwd=2)
```

### Optimal confidence in unequal-variance SDT

The confidence can be computed in the same way (applying Bayes rule).

```{r,fig.height=2.5,fig.width=7,fig.align='center'}
# now apply decision rule
if(length(UE_c)==1){
  resp_yes <- ifelse(x > UE_c,1,0)
}else{
  resp_yes <- ifelse((x <= UE_c[1])|(x>UE_c[2]),1,0)
}


# define a custom function to calculate confidence
confidenceSDT1 <- function(x,resp,dp=d_prime, alpha=0.5){
  conf <- ifelse(
    resp==1,
    dnorm(x,mean=dp,sd=sigmaS)/(dnorm(x,mean=dp,sd=sigmaS)+dnorm(x,mean=0,sd=1)),
    dnorm(x,mean=0,sd=1)/(dnorm(x,mean=dp,sd=sigmaS)+dnorm(x,mean=0,sd=1))
  )
  return(conf)
}

# calculate confidence
confidence <- confidenceSDT1(x, resp_yes)

# put into a dataframe for plotting
d <- data.frame(confidence, x, tar_pres, resp_yes)

# check which simulated responses are correct
d$correct <- ifelse((d$tar_pres==1 & d$resp_yes==1)|(d$tar_pres==0 & d$resp_yes==0),1,0)

# plot
d$tar <- ifelse(d$tar_pres==1,"signal present","signal absent")
d$correct <- ifelse(d$correct==1," correct response","wrong response")
ggplot(d,aes(x=confidence,group=correct,color=correct,fill=correct))+
  geom_histogram(position = 'dodge',aes(y=..density..), binwidth=0.05,alpha=0.9)+
  facet_grid(.~tar)+
  scale_color_manual(values=c("dark green","red"),name="")+
  scale_fill_manual(values=c("dark green","red"),name="")+
  labs(x="confidence level")+
  theme_classic()+
  ggtitle("Unequal-variance SDT")
```

As can be seen from the ROC curve, confidence levels (even estimated optimally using Bayes rule) reveals an asymmetry (again, target present responses are represented by the blue curve). That is, the unequal-variance signal detection theory model predict worse metacognitive sensitivity for "signal absent" responses.

```{r,fig.height=5,fig.width=5,fig.align='center'}
# functions to compute true and false positive rates
TPR <- function(d,th){ sum(d$tar_pres==1 & d$x>th) / sum(d$tar_pres==1)}
FPR <- function(d,th){ sum(d$tar_pres==0 & d$x>th) / sum(d$tar_pres==0)}

# use all the sorted values are possible threshods
thresholds <- sort(d$x)

roc <- data.frame(y=sapply(thresholds, function(th){TPR(d[d$resp_yes==1,],th)}), 
                 x=sapply(thresholds, function(th){FPR(d[d$resp_yes==1,],th)}) )

roc0 <- data.frame(y=sapply(thresholds, function(th){TPR(d[d$resp_yes==0,],th)}), 
                 x=sapply(thresholds, function(th){FPR(d[d$resp_yes==0,],th)}) )

ggplot(roc,aes(x,y))+geom_point(color="blue")+theme_classic()+labs(y="Type-2 hits", x="Type-2 FA") +geom_abline(intercept=0,slope=1,lty=2)+geom_point(data=roc0,color="dark red")+ggtitle("Unequal-variance SDT, Type 2 sensitivity")
```











<!--chapter:end:20-SDT.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Workshops

## Linear multilevel models (LMM) workshop (9th Sept 2022)


Par 1: 
```{r}
knitr::include_url('../workshops/LMM101/LMM_part1.html')
```

Part 2:

```{r}
knitr::include_url('../LMM101/LMM_part1.html')
```

Practicals:

[Exercises with solution (link)](../workshops/LMM101/exercises/exercises_questions.html)

<!--chapter:end:997-workshops.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
# Useful links & resources

## Statistical theory

### [Map of univariate statistical distributions](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html)
![](images/distributions_univariate_map.png){width=75%}




<!--chapter:end:998-links.Rmd-->

```{r include=FALSE, cache=FALSE}
# example R options set globally
options(width = 60)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
  )

# misc
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=FALSE)
options(knitr.table.format = "html") 
```
`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:999-references.Rmd-->

