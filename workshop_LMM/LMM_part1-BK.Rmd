---
title: "Linear Mixed-effects models (LMM) workshop"
subtitle: "(Part 1)"
author: "Matteo Lisi"
institute: "Royal Holloway University of London"
date: "TBC"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: custom/style.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
bibliography: /home/matteo/Dropbox/sync/bib_sync/library.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(mlisi)
```


# Today's program

> 1. Multilevel models from a frequentist perspective
> 2. The Bayesian approach
> 3. Markov Chain Monte Carlo (MCMC)
> 4. Examples

---

# Linear models

> - The dependent variable $y$ is modeled as a weighted combination of the independent variables, plus an additive error $\epsilon$ 
$$y_i=\beta_0 + \beta_1x_{1i} + \ldots +\beta_nx_{ni} + \epsilon_i \\
\epsilon \sim \mathcal{N}\left( 0, \sigma^2 \right)$$

---

# Linear models

> - The dependent variable $y$ is modeled as a weighted combination of the independent variables, plus an additive error $\epsilon$ 
$$\textbf{Y} = \textbf{X}\beta + \epsilon \\
\epsilon \sim \mathcal{N}\left( 0, \sigma^2 \right)$$

---- 

Matrix notation:

$$\bf{Y} = \bf{X}\beta + \epsilon$$
$$\left( \begin{array}{c} y_1 \\ \vdots \\ y_m \end{array} \right) =
\left( \begin{array}{cccc} 
1 & x_{11} & \ldots & x_{1n}\\ 
\vdots & \vdots & \vdots & \vdots\\ 
1 & x_{m1} & \ldots & x_{mn}
\end{array} \right)
\left( \begin{array}{c} \beta_0 \\  \beta_1 \\ \vdots \\ \beta_n \end{array} \right) +
\left( \begin{array}{c} \epsilon_1 \\ \vdots \\ \epsilon_m \end{array} \right)$$

Matrix multiplication: 
$$\left( \begin{array}{cc} a & b \\ c& d \end{array} \right) 
\left( \begin{array}{c} 1 \\ 2 \end{array} \right) = 
1 \left( \begin{array}{c} a \\ c \end{array} \right) + 
2 \left( \begin{array}{c} b \\ d \end{array} \right) =
\left( \begin{array}{c} a + 2b \\ c+ 2d \end{array} \right)$$

---

## Linear mixed-effects models

- 'Classical' linear models are _fixed-effects_ only: 
    - independent variables are all experimental manipulation (they are not random).
    - the only random source of variation is the residual error $\epsilon \sim \mathcal{N}\left( 0, \sigma^2 \right)$.
- However _observations_ (e.g. trials) are often grouped according to _observational cluster_ (e.g. subjects), random samples from a larger population, on which we'd like to make inferences.
- In order to properly generalize from the sample to the population, these variables should be treated as _random effects_. Mixed-effects models allow to do that by explicitly modelling the population distribution.

----

- A model containing both fixed and random effects is usually called a _mixed-effects_ model (but also: hierarchical, multilevel, etc.).
- Random effects are treated as random variations around a population mean. These random variations are usually assumed to have a Gaussian distribution. 
- A simple example: a _random-intercept_ model. Regressions have the same slope in each of the $J$ groups ($j=1,\dots,J$), but random variations in intercept $$y_{ij} = \beta_0 + b_j + \beta_1x_{ij} + \epsilon_{i}\\
\epsilon \sim \mathcal{N}\left( 0, \sigma^2 \right)\\
b \sim \mathcal{N}\left( 0, \sigma_b^2 \right)$$

----

- General formulation in matrix notation
$$\textbf{Y} = \textbf{X}\beta + \textbf{Z}b + \epsilon $$
where $\textbf{X}$ and $\textbf{Z}$ are the known fixed-effects and random-effects regressor matrices.

- The components of the residual error vector $\epsilon \sim \mathcal{N}\left( 0, \sigma^2 \right)$ are assumed to be *i.i.d.* (independent and identically distributed).

- The random-effect components, $b \sim \mathcal{N}\left( 0, \Omega \right)$ are assumed to be normally distributed with mean 0, however they are not necessarily independent (the components $b_j$ can be correlated, and correlations can be estimated). Example:$$\left[ {\begin{array}{c}
{{b_0}}\\
{{b_1}}
\end{array}} \right] \sim\cal N \left( {\left[ {\begin{array}{c}
0 \\ 0 \end{array}} \right],\Omega  = \left[ {\begin{array}{c}
{{\mathop{\rm Var}} \left( b_0 \right)} & {{\mathop{\rm cov}} \left( {{b_0},{b_1}} \right)}\\
{{\mathop{\rm cov}} \left( {{b_0},{b_1}} \right)} & {{\mathop{\rm Var}} \left( b_1 \right)}
\end{array}} \right]} \right)$$
---

## Likelihood function

- Parameters ($\beta$, $\sigma^2$ and $\Omega$) are estimated by maximizing the likelihood function, which is the probability of the data, given the parameters (but treated as a function of the parameters, keeping the data fixed).

- The probability of the data _conditional to the random effects_ is integrated with respect to the marginal density of the random effects, to obtain the marginal density of the data $$
L\left(\beta,\sigma^2,\Omega \mid \text{data}\right) = \int p \left(\text{data} \mid \beta,\sigma^2, b\right) \, p\left(b \mid \Omega\right) \, db
$$

---

## ML vs REML

- The $\textsf{R}$ library `lme4` provides two methods for estimating parameters: Maximum Likelihood (ML) and Restricted Maximum Likelihood (REML).

- ML tend to be biased and underestimate the variance components (e.g. $\Omega$).

- REML provide less biased variance estimates: conceptually can be seen as similar to Bessel's correction for sample variance (using $n-1$ instead of $n$ in the denominator).

---

## Advantages of multilevel models 

- Improved estimates for _repeated sampling_ (e.g. repeated-measures design).
- Improved estimates for _imbalances in sampling_ (e.g. unequal number of trials across subjects).
- Avoid averaging (pre-averaging of data remove variation and can manifacture false confidence).
- Subject-specific standard error is taken into account in group-level estimates.
- Variation among group or individuals is modelled explicitly.
- Outperform classical methods in predictive ability.

## Example 1 `sleepstudy` {.build}

`sleepstudy` is a dataset in the `lme4` package, with reaction times data from 18 subjects that were restricted to 3 hours of sleep for 10 days. 

Questions:

> - how reaction times changes with each sleep-deprived night?
- are individual difference in baseline response times related to individual differences in the effect of sleep deprivation?

```{r}
str(sleepstudy)
```

----

```{r, include=FALSE}
library(ggplot2)
nice_theme <- theme_bw()+theme(text=element_text(family="Helvetica",size=9),panel.border=element_blank(),strip.background = element_rect(fill="white",color="white",size=0),strip.text=element_text(size=rel(0.8)),panel.grid.major.x=element_blank(),panel.grid.major.y=element_blank(),panel.grid.minor=element_blank(),axis.line.x=element_line(size=.4),axis.line.y=element_line(size=.4),axis.text.x=element_text(size=7,color="black"),axis.text.y=element_text(size=7,color="black"),axis.line=element_line(size=.4), axis.ticks=element_line(color="black"))
```

<div class="centered">

```{r, echo=F,fig.height=4,fig.width=7}
ggplot(sleepstudy, aes(x=Days,y=Reaction))+geom_point()+geom_smooth(method="lm",lwd=1)+facet_wrap(~Subject,ncol=9)+nice_theme+scale_x_continuous(breaks=seq(0,10,2))+labs(x="Days of sleep deprivation",y="Mean reaction times [ms]")
```
</div>

----

> - The model we want to fit includes both random slopes and intercept $$
y_{ij} = \beta_0 + b_{0j} + \left( \beta_1 + b_{1j} \right) \times {\rm{Days}}_i + \epsilon_i \\
\epsilon \sim \mathcal{N}\left( 0, \sigma^2 \right) \\
b\sim\cal N \left( 0, \Omega\right)
$$

> - In `lme4` 
```{r, warning=F}
m.1 <- lmer(Reaction ~ 1 + Days + (1 + Days|Subject), sleepstudy)
```

----

```{r}
summary(m.1)
```

----

The function `confint()` allow easily to compute bootstrapped 95% CI of the parameters
```{r}
CI_fixef <- confint(m.1, method="boot", nsim=50, oldNames=F)

print(CI_fixef, digits=2)
```

## {.build}

- Tests of fixed and random effects can also be conducted using likelihood ratio tests, by comparing the likelihood of two _nested_ models.
- If $L_1$ and $L_2$ are the maximised likelihoods of two nested models with $k_1 < k_2$ parameters, the test statistic is $2\text{log}\left( L_2/ L_1 \right)$, which is approximately $\chi^2$ with $k_2 - k_1$ degrees of freedom.
- Example: test if there are substantial variation across subjects in the effect of `Days`
```{r}
# m.1 <- lmer(Reaction ~ 1 + Days + (1 + Days|Subject), sleepstudy)
m.2 <- lmer(Reaction ~ 1 + Days + (1 |Subject), sleepstudy)
anova(m.1, m.2)
```

----
_Shrinkage_: the _predicted_ ${\hat b}_j$ (conditional modes of the random effects) can be seen as a "compromise" between the within-subject estimates and the population mean.

<div class="centered">
```{r, echo=F,fig.height=5.5,fig.width=5}
# analyze shrinking
m.single <- coef(lmList(Reaction ~ Days | Subject, sleepstudy))
par.mixed <- as.matrix(ranef(m.1)$Subject) + repmat(t(as.matrix(fixef(m.1))),18,1)
  
# plot parameters from mixed model and individual fit
plot(m.single[,1], m.single[,2], xlab="Intercept",ylab="Slope",pch=19,cex.lab=1.2,xlim=c(190,310),ylim=c(-5,25),col="dark grey")

# draw ellipse illustrating covariance of random effects
vcov_m.1 <- matrix(as.vector(VarCorr(m.1)$Subject),ncol=2)
mixtools::ellipse(c(mean(par.mixed[,1]), mean(par.mixed[,2])), sigma=vcov_m.1,alpha=0.05, col="grey", lty=2)

points(mean(m.single[,1]), mean(m.single[,2]),pch=19,col="dark grey",cex=2)
points(mean(par.mixed[,1]), mean(par.mixed[,2]),pch=21,col="black",cex=2,lwd=2)
text(m.single[,1], m.single[,2],labels=rownames(m.single),pos=1,cex=0.6)
points(par.mixed[,1], par.mixed[,2])
arrows(m.single[,1], m.single[,2],par.mixed[,1], par.mixed[,2], length=0.1)
legend("bottomright",c("single-subject fits","multilevel model"),pch=c(19,21),col=c("dark grey", "black"),bty="n",cex=0.6)
```
</div>

<div class="notes">
They are sometime called BLUP (_Best Linear Umbiased Predictors_) since they are treated as random variables, not as unknown parameters, and in statistical jargon we say we _predict_ (rather than _estimate_) random variables. A more appropriate term is probably conditional modes of the random effects.
</div>

---

## Diagnostic

As for any linear model, it is important to check that residual errors are well-behaved
<div class="centered">
```{r, echo=F,fig.height=4,fig.width=6}
	# diagnostic plot: visualize the model residuals
	par(mfrow=c(1,2))
	#plot(fitted(m.1),resid(m.1), xlab="fitted values", ylab="residuals") # against fitted values if more than one predictor
	plot(jitter(sleepstudy$Days),resid(m.1), xlab="Days", ylab="residuals") # but here one could plot it agains the preditor Days in this way
	#boxplot(resid(m.1)~sleepstudy$Days, xlab="Days", ylab="residuals")
	abline(h=0,lty=2)
	qqnorm(resid(m.1))
	qqline(resid(m.1),lty=2)
	
```
</div>
